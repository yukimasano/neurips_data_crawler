paper_id	title	link	keywords	abstract
gx7TEqAogg8	Challenging America: Digitized Newspapers as a Source of Machine Learning Challenges	https://openreview.net/forum?id=gx7TEqAogg8	Machine Learning Challenges, Language Modeling, Temporal Classification, Chronicling America	This paper introduces an ML challenge, named ChallAm, based on OCR excerpts from historical newspapers collected on the Chronicling America portal. ChallAm provides a dataset of OCR excerpts, labeled with metadata on their origin and paired with their textual contents retrieved by an OCR tool. Three ML tasks are defined in the challenge: determining the article date, detecting the location of the issue, and deducing a word in a text gap. The challenge is published on the Gonito platform, an evaluation environment for ML tasks, which presents a leader-board of all submitted solutions. Baselines are provided in Gonito for all three tasks of the challenge.
yYQuqGcxFvb	GitTables: A Large-Scale Corpus of Relational Tables	https://openreview.net/forum?id=yYQuqGcxFvb	relational tables, semantic annotations, table understanding, semantic column type detection, github, dbpedia, schema.org	The practical success of deep learning has sparked interest in improving relational table tasks, like data search, with models trained on large table corpora. Existing corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need additional resources with tables that resemble relational database tables.               Here we introduce GitTables, a corpus of currently 1.7M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 20M tables. We annotate table columns in GitTables with more than 2K different semantic types from Schema.org and DBpedia. Our column annotations consist of semantic types, hierarchical relations, range types and descriptions. The corpus is available at https://gittables.github.io.              Our analysis of GitTables shows that its structure, content, and topical coverage differ significantly from existing table corpora. We evaluate our annotation pipeline on hand-labeled tables from the T2Dv2 benchmark and find that our approach provides results on par with human annotations. We demonstrate a use case of GitTables by training a semantic type detection model on it and obtain high prediction accuracy. We also show that the same model trained on tables from the Web generalizes poorly.
xVQMrDLyGst	Contemporary Symbolic Regression Methods and their Relative Performance	https://openreview.net/forum?id=xVQMrDLyGst	symbolic regression, benchmarks, physics, differential equations	Many promising approaches to symbolic regression have been presented in recent years, yet progress in the field continues to suffer from a lack of uniform, robust, and transparent benchmarking standards. In this paper, we address this shortcoming by introducing an open-source, reproducible benchmarking platform for symbolic regression. We assess 14 symbolic regression methods and 7 machine learning methods on a set of 252 diverse regression problems.  Our assessment includes both real-world datasets with no known model form as well as ground-truth benchmark problems, including physics equations and systems of ordinary differential equations. For the real-world datasets, we benchmark the ability of each method to learn models with low error and low complexity relative to state-of-the-art machine learning methods. For the synthetic problems, we assess each method's ability to find exact solutions in the presence of varying levels of noise. Under these controlled experiments, we conclude that the best performing methods for real-world regression combine genetic algorithms with parameter estimation and/or semantic search drivers. When tasked with recovering exact equations in the presence of noise, we find that deep learning and genetic algorithm-based approaches perform similarly. We provide a detailed guide to reproducing this experiment and contributing new methods, and encourage other researchers to collaborate with us on a common and living symbolic regression benchmark.
lwlkxYsGDi	DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction	https://openreview.net/forum?id=lwlkxYsGDi	Neural networks, protein interface prediction, geometric deep learning, computational biology	How and where proteins interface with one another can ultimately impact the proteins' functions along with a range of other biological processes. As such, precise computational methods for protein interface prediction (PIP) come highly sought after as they could yield significant advances in drug discovery and design as well as protein function analysis. However, the traditional benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a modest 230 complexes for training, validating, and testing different machine learning algorithms. In this work, we expand on a dataset recently introduced for this task, the Database of Interacting Protein Structures (DIPS), to present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for geometric deep learning of protein interfaces. The previous version of DIPS contains only the Cartesian coordinates and types of the atoms comprising a given protein complex, whereas DIPS-Plus now includes a plethora of new residue-level features including protrusion indices, half-sphere amino acid compositions, and new profile hidden Markov model (HMM)-based sequence features for each amino acid, giving researchers a large, well-curated feature bank for training protein interface prediction methods. We demonstrate through rigorous benchmarks that pre-training existing state-of-the-art (SOTA) models for PIP on DIPS-Plus yields SOTA results, surpassing the performance of all other models trained on residue-level and atom-level encodings of protein complexes to date.
HQ-6VDYUxGn	PROCAT: Product Catalogue Dataset for Implicit Clustering, Permutation Learning and Structure Prediction	https://openreview.net/forum?id=HQ-6VDYUxGn	set-to-sequence, structure prediction, product catalog, product catalogue, product brochure, permutation learning	In this dataset paper we introduce PROCAT, a novel e-commerce dataset containing expertly designed product catalogues consisting of individual product offers grouped into complementary sections. We aim to address the scarcity of existing datasets in the area of set-to-sequence machine learning tasks, which involve complex structure prediction. The task's difficulty is further compounded by the need to place into sequences rare and previously-unseen instances, as well as by variable sequence lengths and substructures, in the form of diversely-structured catalogues. PROCAT provides catalogue data consisting of over 1.5 million set items across a 4-year period, in both raw text form and with pre-processed features containing information about relative visual placement. In addition to this ready-to-use dataset, we include baseline experimental results on a proposed benchmark task from a number of joint set encoding and permutation learning model architectures.
kOxP7Fbeduy	SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments	https://openreview.net/forum?id=kOxP7Fbeduy	Monocular Depth Prediction, Long-term Visual Perception and Localization, Dataset and Benchmark	Different environments pose a great challenge on the outdoor robust visual perception for long-term autonomous driving and the generalization of learning-based algorithms on different environmental effects is still an open problem. Althoughmonocular depth prediction has been well studied recently, there is few work focusing on the robust learning-based depth prediction across different environments,e.g.changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the first cross-season monocular depth prediction dataset and benchmarkSeasonDepth (available on https://seasondepth.github.io/) is built based on CMU Visual Localizationdataset. To benchmark the depth estimation performance under different environments, we investigate representative and recent state-of-the-art open-source supervised, self-supervised and domain adaptation depth prediction methods fromKITTIbenchmark using several newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset, the influence of multiple environments on performance and robustness is analyzed both qualitatively and quantitatively, showing that the long-term monocular depth prediction is far from solved even with fine-tuning. We further give promising avenues that self-supervised training and stereo geometry constraint help to enhance the robustness to changing environments.
6lE4dQXaUcb	CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation	https://openreview.net/forum?id=6lE4dQXaUcb		Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.
KBbxt3JGn0Y	One Million Scenes for Autonomous Driving: ONCE Dataset	https://openreview.net/forum?id=KBbxt3JGn0Y	autonomous driving, 3D object detection, dataset, benchmark	Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (\eg nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at \href{https://once-for-auto-driving.github.io/index.html}{http://www.once-for-auto-driving.com}.
q7XJj9_egih	Particulate Matter Dataset Collected with Vehicle Mounted IoT Devices in Delhi-NCR	https://openreview.net/forum?id=q7XJj9_egih	air pollution, particulate matter, mobile monitoring, interpolation, anomaly detection	Air pollution is one of the biggest concerns faced by developing countries like India and the world at large. The capital of India, Delhi and the National Capital Region (NCR), sees life threatening air pollution levels. This paper presents a new Particulate Matter (PM) dataset for Delhi-NCR, which contains PM data recorded over three months from November 2020 to January 2021 over an area spanning 559 square Kms. The data has been collected using vehicle-mounted mobile sensors in collaboration with the Delhi Integrated Multi-Modal Transit System (DIMTS) buses. The 13 bus dataset has been compared with the data over the same period obtained from the pre-existing static sensors, which the buses pass by. Several Machine Learning (ML) problems have been outlined, that can be studied using this dataset, two of which, spatio-temporal interpolation and anomaly detection in IoT networks are detailed in this paper. The dataset is public at https://www.cse.iitd.ac.in/pollutiondata, along with appropriate documentation. We will keep augmenting the website as new data get collected, with more buses and other pollutant sensors (SOx, NOx, COx) added to our deployment in future.
Pq8FBz0gZHY	Generating Datasets of 3D Garments with Sewing Patterns	https://openreview.net/forum?id=Pq8FBz0gZHY	Garment Dataset, Sewing Patterns, Scan Imitation, 3D Deep Learning	Garments are ubiquitous in both real and many of the virtual worlds. They are highly deformable objects, exhibit an immense variety of designs and shapes, and yet, most garments are created from a set of regularly shaped flat pieces. Exploration of garment structure presents a peculiar case for an object structure estimation task and might prove useful for downstream tasks of neural 3D garment modeling and reconstruction by providing strong prior on garment shapes. To facilitate research in these directions, we propose a method for generating large synthetic datasets of 3D garment designs and their sewing patterns. Our method consists of a flexible description structure for specifying parametric sewing pattern templates and the automatic generation pipeline to produce garment 3D models with little-to-none manual intervention. To add realism, the pipeline additionally creates corrupted versions of the final meshes that imitate artifacts of 3D scanning.              With this pipeline, we created the first large-scale synthetic dataset of 3D garment models with their sewing patterns. The dataset contains more than 20000 garment design variations produced from 19 different base types. Seven of these garment types are specifically designed to target evaluation of the generalization across garment sewing pattern topologies.
hhKA5k0oVy5	Variance-Aware Machine Translation Test Sets	https://openreview.net/forum?id=hhKA5k0oVy5	machine translation, evaluation, test sets	We release 70 small and discriminative test sets for machine translation (MT) evaluation called variance-aware test sets (VAT), covering 35 translation directions from WMT16 to WMT20 competitions. VAT is automatically created by a novel variance-aware filtering method that filters the indiscriminative test instances of the current MT benchmark without any human labor. Experimental results show that VAT outperforms the original WMT benchmark in terms of the correlation with human judgment across mainstream language pairs and test sets. Further analysis on the properties of VAT reveals the challenging linguistic features (e.g., translation of low-frequency words and proper nouns) for the competitive MT systems, providing guidance for constructing future MT test sets. The test sets and the code for preparing variance-aware MT test sets are freely available at https://github.com/NLP2CT/Variance-Aware-MT-Test-Sets.
cbFfF4g9fIy	NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search	https://openreview.net/forum?id=cbFfF4g9fIy	automated machine learning, neural architecture search	Most existing neural architecture search (NAS) benchmarks and algorithms prioritize performance on well-studied tasks, focusing on computer vision datasets such as CIFAR and ImageNet. However, the applicability of NAS approaches in other areas is not adequately understood. In this paper, we present NAS-Bench-360, a benchmark suite for evaluating state-of-the-art NAS methods on less-explored datasets. To do this, we organize a diverse array of tasks, from classification of simple deformations of natural images to predicting protein folding and partial differential equation (PDE) solving. Our evaluation pipeline compares architecture search spaces of different flavors, and reveals varying performance on different tasks, providing baselines for further use. All data and reproducible evaluation code are open-source and publicly available. The results of our evaluation show that current state-of-the-art NAS methods often struggle to compete with simple baselines and human-designed architectures on the majority of tasks in our benchmark. At the same time, they can be quite effective on a few individual, understudied tasks. This demonstrates the importance of evaluation on diverse tasks to better understand the usefulness of different approaches to architecture search and automation.
hwjnu6qW7E4	Personalized Benchmarking with the Ludwig Benchmarking Toolkit	https://openreview.net/forum?id=hwjnu6qW7E4	benchmarking, benchmarking tools, benchmarking toolkits, model benchmarking, benchmarks	The rapid proliferation of machine learning models across domains and deployment settings has given rise to various communities (e.g. industry practitioners) which seek to benchmark models across tasks and objectives of personal value. Unfortunately, these users cannot use standard benchmark results to perform such value-driven comparisons, as traditional benchmarks evaluate models on a single objective (e.g. average accuracy) and don’t facilitate a standardized training framework that controls for confounding variables (e.g. computational budget), making fair comparisons difficult. To address these challenges, we introduce the open-source Ludwig Benchmarking Toolkit (LBT), a personalized benchmarking toolkit for running end-to-end benchmark studies (from hyperparameter optimization to evaluation) across an easily extensible set of tasks, deep learning models, datasets and evaluation metrics. LBT provides a configurable interface for customizing evaluation and controlling training, a standardized training framework for eliminating confounding variables, and support for multi-objective evaluation. We demonstrate how LBT can be used to create personalized benchmark-studies with a large-scale comparative analysis for text classification across 7 models and 9 datasets. We explore the trade-offs between inference latency and performance, relationships between dataset attributes and performance, and the effects of pretraining on convergence and robustness, showing how LBT can be used to satisfy various benchmarking objectives.
HrhaC-bLC5U	Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers	https://openreview.net/forum?id=HrhaC-bLC5U	spoken language understanding, speech recognition, open source data	This paper introduces Timers and Such, a new open source dataset of spoken English commands for common voice control use cases involving numbers. We describe the gap in existing spoken language understanding datasets that Timers and Such fills, the design and creation of the dataset, and experiments with a number of ASR-based and end-to-end baseline models, the code for which has been made available as part of the SpeechBrain toolkit.
4lvu1B4Y0E	ComSum: Commit Messages Summarization and Meaning Preservation	https://openreview.net/forum?id=4lvu1B4Y0E	Data set, dataset, summarization, abstractive, commits, subjects, developers, programming, comsum, cumsum, ComSum, natural language processing, data set	We present ComSum, a data set of 7 million commit messages for text summarization. When documenting commits, software code changes, both a message and its summary are posted. We gather and filter those to curate developers' work summarization data set.       Along with its growing size, practicality and challenging language domain, the data set benefits from the living field of empirical software engineering.        As commits follow a typology, we propose to not only evaluate outputs by Rouge, but by their meaning preservation.
Ud1K-l71AI2	Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management	https://openreview.net/forum?id=Ud1K-l71AI2	Bias, Pain, Medicine, Healthcare, NLP, QA	Recent advances in Natural Language Processing (NLP), and specifically automated Question Answering (QA) systems, have demonstrated both impressive linguistic fluency and a pernicious tendency to reflect social biases. In this study, we introduce Q-Pain, a dataset for assessing bias in medical QA in the context of pain management, one of the most challenging forms of clinical decision-making. Along with the dataset, we propose a new, rigorous framework, including a sample experimental design, to measure the potential biases present when making treatment decisions. We demonstrate its use by assessing two reference Question-Answering systems, GPT-2 and GPT-3, and find statistically significant differences in treatment between intersectional race-gender subgroups, thus reaffirming the risks posed by AI in medical settings, and the need for datasets like ours to ensure safety before medical AI applications are deployed.
izzQAL8BciY	MultiBench: Multiscale Benchmarks for Multimodal Representation Learning	https://openreview.net/forum?id=izzQAL8BciY	multimodal learning, representation learning, robustness, complexity	Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning spanning innovations in fusion paradigms, optimization objectives, and training approaches. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized implementations, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.
IO7jcf63iDI	WaveFake: A Data Set to Facilitate Audio DeepFake Detection	https://openreview.net/forum?id=IO7jcf63iDI	generative model, audio DeepFake detection, signal processing, data set	Deep generative modeling has the potential to cause significant harm to society. Recognizing this threat, a magnitude of research into detecting so-called ``"DeepFakes" has emerged. This research most often focuses on the image domain, while studies exploring generated audio signals have---so-far---been neglected. In this paper we make three key contributions to narrow this gap. First, we provide researchers with an introduction to common signal processing techniques used for analyzing audio signals. Second, we present a novel data set, for which we collected audio samples from five different network architectures, across two languages. Finally, we supply practitioners with two baseline models, adopted from the signal processing community, to facilitate further research in this area.
dA2Q8CfmGpp	BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling	https://openreview.net/forum?id=dA2Q8CfmGpp	End-to-end, Task-oriented Dialogue, Bilingual	Task-oriented dialogue (ToD) benchmarks provide an important avenue to measure progress and develop better conversational agents. However, existing datasets for end-to-end ToD modeling are limited to a single language, hindering the development of robust end-to-end ToD systems for multilingual countries and regions. Here we introduce BiToD, the first bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual ToD systems and cross-lingual transfer learning approaches. We provide state-of-the-art baselines under three evaluation settings (monolingual, bilingual, and cross-lingual). The analysis of our baselines in different settings highlights 1) the effectiveness of training a bilingual ToD system comparing to two independent monolingual ToD systems, and 2) the potential of leveraging a bilingual knowledge base and cross-lingual transfer learning to improve the system performance in the low resource condition.
FNUijta5T-1	HumBugDB: a large-scale acoustic mosquito dataset	https://openreview.net/forum?id=FNUijta5T-1	Acoustic machine learning, audio event detection, audio classification, mosquito detection, Bayesian deep learning	This paper presents the first large-scale multi-species dataset of acoustic recordings of mosquitoes tracked continuously in free flight. Mosquitoes are well-known carriers of diseases such as malaria, dengue and yellow fever. The motivation for collecting such a large dataset comes from the need to gather information, help predict outbreaks, and inform data-driven policy. The task of detecting mosquitoes from their wingbeats is made challenging due to the difficulty in collecting recordings from realistic scenarios. To address this, as part of the HumBug project, we have conducted global experiments  to record mosquitoes ranging from those bred indoors in culture cages to mosquitoes captured in the wild. As a result, the audio recordings vary widely in signal-to-noise ratio and contain a broad range of indoor and outdoor background environments from Tanzania, Thailand, Kenya, the USA and the UK. The audio recordings have been labelled by domain experts, aided by Bayesian neural networks. As a result, we present 20 hours of mosquito audio recordings expertly labelled with tags precise in time, of which 18 hours are annotated from 36 different species. We provide our data from a regularly maintained database, which captures important metadata such as the capture method, age, feeding status and gender of the mosquitoes. Additionally, we provide code to extract features and train Bayesian convolutional neural networks that can distinguish mosquito sounds from their corresponding background. Our contribution is to provide a dataset that is both challenging to machine learning researchers focusing on acoustic identification, and critical to entomologists, geo-spatial modellers and other domain experts to understand mosquito behaviour, model their distribution, and manage the threat they pose to humans.
DSo2Zteb9l8	NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning	https://openreview.net/forum?id=DSo2Zteb9l8	offline reinforcement learning, offline evaluation, benchmarks	Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running an overly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be carefully evaluated before deployment.       In this paper, we present a Near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for offline policy evaluation. We evaluate recent SOTA offline RL algorithms on NeoRL, through both online evaluation and purely offline evaluation. The empirical results demonstrate that the tested offline RL algorithms become less competitive to BC on many datasets, and the current offline policy evaluation methods can hardly select truly effective policies. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.
ip0FhVivoX0	Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening	https://openreview.net/forum?id=ip0FhVivoX0	Surrogate models, virtual screening, drug discovery, SARS-CoV-2	We propose a benchmark to study surrogate model accuracy for protein-ligand docking. We share a dataset consisting of 200 million 3D complex structures and 2D structure scores across a consistent set of 13 million ``in-stock'' molecules over 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work shows surrogate docking models have six orders of magnitude more throughput than standard docking protocols on the same supercomputer node types. We demonstrate the power of high-speed surrogate models by running each target against 1 billion molecules in under a day (50k predictions per GPU seconds). We showcase a workflow for docking utilizing surrogate ML models as a pre-filter. Our workflow is ten times faster at screening a library of compounds than the standard technique, with an error rate less than 0.01% of detecting the underlying best scoring 0.1\% of compounds. Our analysis of the speedup explains that to screen more molecules under a docking paradigm, another order of magnitude speedup must come from model accuracy rather than computing speed (which, if increased, will not anymore alter our throughput to screen molecules). We believe this is strong evidence for the community to begin focusing on improving the accuracy of surrogate models to improve the ability to screen massive compound libraries 100x or even 1000x faster than current techniques.
3_hgF1NAXU7	Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription	https://openreview.net/forum?id=3_hgF1NAXU7	crowdsourcing, speech recognition, aggregation	Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life.        Crowdsourcing has become one of the standard tools for cheap and time-efficient data collection for simple problems such as image classification: thanks in large part to advances in research on aggregation methods.       However, the applicability of crowdsourcing to more complex tasks  (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing advanced aggregation methods is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CrowdSpeech --- the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of collecting high-quality datasets using crowdsourcing: we develop a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VoxDIY --- a counterpart of CrowdSpeech for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.
Nc2uduhU9qa	EEGEyeNet: a Simultaneous Electroencephalography and Eye-tracking Dataset and Benchmark for Eye Movement Prediction	https://openreview.net/forum?id=Nc2uduhU9qa	electroencephalography, eye-tracking, deep learning, dataset, benchmark	We present a new dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. Our dataset, EEGEyeNet, consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms. Using this dataset, we also propose a benchmark to evaluate gaze prediction from EEG measurements. The benchmark consists of three tasks with an increasing level of difficulty: left-right, angle-amplitude and absolute position. We run extensive experiments on this benchmark in order to provide solid baselines, both based on classical machine learning models and on large neural networks. We release our complete code and data and provide a simple and easy-to-use interface to evaluate new methods.
Rtquf4Jk0jN	ReaSCAN: Compositional Reasoning in Language Grounding	https://openreview.net/forum?id=Rtquf4Jk0jN	compositional generalization, compositional reasoning, language grounding	The ability to compositionally map language to referents, relations, and actions is an essential component of language understanding. The recent gSCAN dataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the capacity of models to learn this kind of grounding in scenarios involving navigational instructions. However, we show that gSCAN's highly constrained design means that it does not require compositional interpretation and that many details of its instructions and scenarios are not required for task success. To address these limitations, we propose ReaSCAN, a benchmark dataset that builds off gSCAN but requires compositional language interpretation and reasoning about entities and relations. We assess two models on ReaSCAN: a multi-modal baseline and a state-of-the-art graph convolutional neural model. These experiments show that ReaSCAN is substantially harder than gSCAN for both neural architectures. This suggests that ReaSCAN can serve as a valuable benchmark for advancing our understanding of models' compositional generalization and reasoning capabilities.
rX_fl1Gd0k4	Karenina: Modeling the Complexity of Negative Emotions to Better Serve Industry Goals	https://openreview.net/forum?id=rX_fl1Gd0k4	emotion analysis, sentiment analysis, industrial applications	Sentiment analysis systems are widely applied in industry, but standard formulations of the task (e.g., positive/negative/neutral classification) are often not well aligned with real-world goals. For instance, in customer support contexts, negative labels dominate due to the nature of the work, and different negative emotions call for different solutions. To help address this issue, we introduce Karenina, a labeled dataset of 25K consumer healthcare experience comments with labels that support standard sentiment distinctions but also allow for a breakdown into six negative emotions: confused, disappointed, frustrated, angry, stressed, and worried. Each text has 1-4 emotion labels, with over 90% of examples having at least 2 labels. We define strong baselines for this dataset, we seek to motivate a flexible approach to evaluation that takes into account the variable costs for different mistakes in different industrial contexts, and we report on some illustrative analyses using Karenina to understand customer experiences.
R07XwJPmgpl	OmniPrint: A Configurable Printed Character Synthesizer	https://openreview.net/forum?id=R07XwJPmgpl	ocr, meta-learning, synthesizer	We introduce OmniPrint, a synthetic data generator of isolated printed characters, geared toward machine learning research. It draws inspiration from famous datasets such as MNIST, SVHN and Omniglot, but it offers the capability of generating a wide variety of printed characters from various languages, fonts and styles, with customized distortions. Use cases include: image classification benchmarks, data augmentation, calibration, bias detection/compensation, meta-learning and transfer learning, modular learning from decomposable/separable problems, recognition of printed characters in the wild, and creation of captchas. We include 935 fonts from 27 scripts and many types of distortions. As a proof of concept, we show an example of use case for meta-learning, which is a dataset designed for the upcoming MetaDL challenge, accepted as part of the NeurIPS 2021 competition track. OmniPrint will be open-sourced after the MetaDL challenge.
fE_gwAAKM7O	Natural Adversarial Objects	https://openreview.net/forum?id=fE_gwAAKM7O	robustness, dataset, adversarial example	Although state-of-the-art object detection methods have shown compelling performance, models often are not robust to adversarial attacks and out-of-distribution data.              We introduce a new dataset, Natural Adversarial Objects (NAO), to evaluate the robustness of object detection models. NAO contains 7,936 images and 13,604 objects that are unmodified, but cause state-of-the-art detection models to misclassify with high confidence. The mean average precision (mAP) of EfficientDet-D7 drops 68.3\% when evaluated on NAO compared to the standard MSCOCO validation set.              We investigate why examples in NAO are difficult to detect and classify. Experiments of shuffling image patches reveal that models are overly sensitive to local texture. Additionally, using integrated gradients and background replacement, we find that the detection model is reliant on pixel information within the bounding box, and insensitive to the background context when predicting class labels.
VtqyY2dvE6h	MDP Playground: A Design and Debug Testbed for Reinforcement Learning	https://openreview.net/forum?id=VtqyY2dvE6h	Reinforcement learning, Core issues, Efficiency, Reproducibility, Dimensions of hardness, OpenAI Gym, Benchmarks	We present \emph{MDP Playground}, an efficient testbed for Reinforcement Learning (RL) agents with \textit{orthogonal} dimensions that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of dimensions, including \textit{delayed rewards}, \textit{rewardable sequences}, \textit{density of rewards}, \textit{stochasticity}, \textit{image representations}, \textit{irrelevant features}, \textit{time unit}, \textit{action range} and more. We define a parameterised collection of fast-to-run toy environments in \textit{OpenAI Gym} by varying these dimensions and propose to use these for the initial design and development of agents. We also provide wrappers that inject these dimensions into complex environments from \textit{Atari} and \textit{Mujoco} to allow for evaluating agent robustness. We further provide various example use-cases and instructions on how to use \textit{MDP Playground} to design and debug agents. We believe that \textit{MDP Playground} is a valuable testbed for researchers designing new, adaptive and intelligent RL agents and those wanting to unit test their agents.
wBOBL5b-aa9	DermX: a Dermatological Diagnosis Explainability Dataset	https://openreview.net/forum?id=wBOBL5b-aa9	dataset, explainability, dermatology, diagnosis, segmentation	In this paper, we introduce DermX: a novel dermatological diagnosis and explanations dataset annotated by eight board-certified dermatologists. To date, public datasets for dermatological applications have been limited to diagnosis and lesion segmentation, while validation of dermatological explainability has been limited to visual inspection. As such, this work is a first release of a dataset providing gold standard explanations for dermatological diagnosis to enable a quantitative evaluation of ConvNet explainability. DermX consists of 525 images sourced from two public datasets, DermNetNZ and SD-260, spanning six of the most prevalent skin conditions. Each image was enriched with diagnoses and diagnosis explanations by three dermatologists. Supporting explanations were collected as 15 non-localisable characteristics, 16 localisable characteristics, and 23 additional terms.  Dermatologists manually segmented localisable characteristic and described them with additional terms.  We showcase a possible use of our dataset by benchmarking the explainability of two ConvNet architectures, ResNet-50 and EfficientNet-B4,trained on an internal skin lesion dataset and tested on DermX. ConvNet visualisations are obtained through gradient-weighted class-activation map (Grad-CAM), a commonly used model visualisation technique. Our analysis reveals EfficientNet-B4 as the most explainable between the two. Thus, we prove that DermX can be used to objectively benchmark the explainability power of dermatological diagnosis models. The dataset is available at https://github.com/ralucaj/dermx.
BwzYI-KaHdr	PASS: An ImageNet replacement for self-supervised pretraining without humans	https://openreview.net/forum?id=BwzYI-KaHdr	computer vision, self-supervised learning, privacy, dataset	Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.
kUkp7WdUny9	SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving	https://openreview.net/forum?id=kUkp7WdUny9	autonomous driving, object detection, dataset, benchmark, self-supervised learning, semi-supervised learning	Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest benchmark to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (e.g., detection) trained using extensive annotated data to ensure the safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (e.g., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent powerful advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo) either provides only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale Object Detection benchmark for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes. We provide extensive experiments and deep analyses of existing supervised state-of-the-art detection models, popular self-supervised and semi-supervised approaches, and some insights about how to develop future models. We show that SODA10M can serve as a promising pre-training dataset for different self-supervised learning methods, which gives superior performance when finetuning autonomous driving downstream tasks. This benchmark will be used to hold the ICCV2021 SSLAD challenge. The data and more up-to-date information have been released at https://soda-2d.github.io.
pl6xnx3jZMh	ExpMRC: Explainability Evaluation for Machine Reading Comprehension	https://openreview.net/forum?id=pl6xnx3jZMh	reading comprehension, question answering, explainable AI	Achieving human-level performance on some Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, it is necessary to provide both answer prediction and its explanation to further improve the MRC system's reliability, especially for real-life applications. In this paper, we propose a new benchmark called ExpMRC for evaluating the explainability of the MRC systems. ExpMRC contains four subsets, including SQuAD, CMRC 2018, RACE + , and C 3 , with additional annotations of the answer's evidence. The MRC systems are required to give not only the correct answer but also its explanation. We use state-of-the-art pre-trained language models to build baseline systems and adopt various unsupervised approaches to extract evidence without a human-annotated training set. The experimental results show that these models are still far from human performance, suggesting that the ExpMRC is challenging.
ogNcxJn32BZ	Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing	https://openreview.net/forum?id=ogNcxJn32BZ	NLP, explainability, explainable NLP, explainable AI, XAI	Explainable Natural Language Processing (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 61 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.
R_CuaMJKvDs	The Cross-environment Hyperparameter Setting Benchmark for Reinforcement Learning	https://openreview.net/forum?id=R_CuaMJKvDs	Reinforcement Learning, benchmark	This paper introduces a new benchmark, the Single Hyperparameter Benchmark, that allows comparison of RL algorithms across environments using only a single hyperparameter, encouraging algorithmic development which is insensitive to hyperparameters. We demonstrate that the benchmark is robust to statistical noise and obtains qualitatively similar results across repeated applications, even when using a small number of samples. This robustness makes the benchmark computationally cheap to apply, allowing statistically sound insights at a low cost. To demonstrate the applicability of the SHB to modern RL algorithms on challenging environments, we provide a novel empirical study of an open question in the continuous control literature. We show, with high confidence, that there is no meaningful difference in performance between Ornstein-Uhlenbeck noise and uncorrelated Gaussian noise for exploration with the DDPG algorithm across the entire DMControl suite.
QkOBP-aD1qA	QAConv: Question Answering on Informative Conversations	https://openreview.net/forum?id=QkOBP-aD1qA	question answering, conversational AI, dataset	This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations, including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge.  In total, we collect 34,204 QA pairs, including multi-span and unanswerable questions, from 10,259 selected conversations with both human-written and machine-generated questions. We segment long conversations into chunks and use a question generator and a dialogue summarizer as auxiliary tools to collect multi-hop questions. The dataset has two testing scenarios, chunk mode and full mode, depending on whether the grounded chunk is provided or retrieved from a large pool of conversations.  Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot ability and tend to predict our questions as unanswerable. Finetuning such systems on our corpus can significantly improve up to 23.6\% and 13.6\% in both chunk mode and full mode, respectively.
skFwlyefkWJ	MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research	https://openreview.net/forum?id=skFwlyefkWJ		The progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents.However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity.
TAdzPkgnnV8	The Tarteel Dataset: Crowd-Sourced and Labeled Quranic Recitation	https://openreview.net/forum?id=TAdzPkgnnV8	natural language processing, speech recognition, recitation, Arabic, Quran, crowd-sourced, diverse, dataset, Islam	We propose a standard schema for paired Quranic audio and text datasets. We describe the collection, labeling, and validation of the Tarteel recitation dataset, the first large-scale dataset of Quranic recitation and accompanying Arabic text collected in a crowd-sourced manner. The dataset contains 25,000 audio clips totalling 67.39 hours of audio and represents a wide variety of recitation styles, proficiencies, and speeds. The data were collected over a period of six months from over 1,200 unique individuals of different ages, genders, and ethnicities. We describe the composition of the data and contributors, describe in detail how the data was collected and processed, and give some baseline performance for preliminary machine learning algorithms that were trained and evaluated on the dataset.
PQldd-oUHa8	“There will be consequences!” Datasets and Benchmarks towards Causal Relation Extraction and Societal Event Forecasting	https://openreview.net/forum?id=PQldd-oUHa8		In this paper, we discuss the creation of datasets (from which future benchmarks and other derived datasets can be created), geared towards causal relation extraction from a given context (obtained from the Google Natural Questions dataset) and event forecasting, especially that of making predictions of effects of societal events based on past iterations of similar events and more, obtained from Wikipedia and WikiNews. We introduce multiple novel datasets in this work:- 1.) Four named versions of benchmarks towards extracting verbatim phrasal causes and effects based on contextual than commonsense reasoning from Google’s Natural Questions dataset; two being denoted as the final set, called the Final Evaluation versions set of the Cause-Effect-Context from Natural Questions (NQ-CE) 2.) Three datasets of impacts and benchmarks from Wikipedia, so that, given some query event X, we try to answer the question, "what will follow?"; namely for the COVID-19, 9/11 and the Hurricane Sandy usecases, collectively referred to as the Dataset of Significant Societal Events and their Impacts from Wikipedia (SSIW) and two collections of significant societal events from Wikipedia year pages, called the Significant Societal Events from Wikipedia Year Pages (SSWY). 3.) One dataset of events from WikiNews, called the Event Consequence Collections from WikiNews (ECCW); extracting source events, their consequences and relevant and best matching semantically related non consequences for thirty-nine category pages in WikiNews after having filtered them using a combination of automated and manual filtering approaches from over 10k category pages. Future work, including evaluating benchmarks and creating datasets with more categories and types of events, along with creating derived benchmarks from any created datasets is planned.
t1kTABPcp9s	Indian-CT, Dataset and Analysis of Chest CT Scans of COVID-19 Patients Using Light-weight CNN	https://openreview.net/forum?id=t1kTABPcp9s	COVID-19, Convolutional Neural Networks, Computed Tomography Scans, Indian-CT	An Indian dataset of chest Computed Tomography (CT) images from COVID-19 patients, Indian-CT, with demographics of the participants has been collected and curated for aiding in the diagnosis of COVID-19 and other chest CT analysis tasks. The Indian-CT dataset now consists of 6174 images from 147 patients and can be used for validating the generalizability of models on Indian patient data. This dataset will be updated to include more data. Here we also propose a lightweight Convolutional Neural Network (CNN) model for the diagnosis of COVID-19. The proposed model classifies chest CT scans into three classes, viz., Normal, non-Covid Pneumonia and COVID-19. The model has been trained and validated on publicly available dataset COVIDx-CT dataset [1]. Performance of the model is evaluated on both COVIDx-CT and Indian-CT datasets and is observed to be comparable. The accuracy of the model is slightly lower on Indian-CT dataset which is not surprising as it is an external test set. However, it is worth noting that the model’s performance is comparable even on an external cohort. The proposed lightweight model for diagnosing COVID-19 is well suited for a clinical setting. However, the model is still a prototype and needs more rigorous testing and re-calibrations before using it for clinical diagnosis. The dataset will be made available at http://aimedhub.iiit.ac.in/datasets/gandhi-hospital-covid-dataset in the future.
VdvDlnnjzIN	Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation	https://openreview.net/forum?id=VdvDlnnjzIN	reinforcement learning, physics, differentiable, jax, robotics	We present Brax, an open source library for \textbf{r}igid \textbf{b}ody simulation with a focus on performance and parallelism on accelerators, written in JAX.  We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine.  Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators.  Finally, we include notebooks that facilitate training of performant policies on common MuJoCo-like tasks in minutes.
iBLHqLgbRn	VISIOCITY: A New Benchmarking Dataset and Evaluation Framework Towards Realistic Video Summarization	https://openreview.net/forum?id=iBLHqLgbRn	video summarization, dataset, benchmark, evaluation	Automatic video summarization is still an unsolved problem due to several challenges. The currently available datasets either have very short videos or have a few long videos of only a particular type. We introduce a new benchmarking video dataset called VISIOCITY (VIdeo SummarIzatiOn based on Continuity, Intent and DiversiTY) which comprises of longer videos across six different categories with dense concept annotations capable of supporting different flavors of video summarization and other vision problems. Secondly, human reference summaries necessary for supervised video summarization techniques are difficult to obtain, especially for long videos. We explore strategies to automatically generate multiple reference summaries from indirect ground truth present in VISIOCITY. We also present a study of different desired characteristics of a good summary and argue that evaluating a summary against one or more human summaries and using a single measure has its shortcomings. We propose an evaluation framework for better quantitative assessment of summary quality which is closer to human judgment. We present insights into how a model can be enhanced to yield better summaries and demonstrate the effectiveness of our recipe in doing so as compared to some of the representative state of the art techniques tested on VISIOCITY. We release VISIOCITY (https://visiocity.github.io/) and invite researchers to test their algorithms on VISIOCITY benchmark.
qMxVrCug4rI	Chest ImaGenome Dataset for Clinical Reasoning	https://openreview.net/forum?id=qMxVrCug4rI		Despite the progress in the automatic detection of radiologic findings in chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few human-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global "weak" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe 242072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: 1) 1,256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, 2) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as 3) a manually annotated gold standard scene graph dataset from 500 unique patients.
Mop0QMT5yei	A Dataset of Discovering Drug-Target Interaction from Biomedical Literature	https://openreview.net/forum?id=Mop0QMT5yei		As millions of papers come out every year in the biomedical domain, automatic knowledge discovery (KD) from biomedical literature becomes an urgent demand in the industry. While KD in the biomedical domain attracts much research attention in recent years, the lack of benchmark datasets significantly hinders its progress. In this work, we create a dataset, KD-DTI, for discovering <drug, target, interaction> triplets from literature, which is one of the most important KD tasks in the biomedical domain. KD-DTI contains 14k unique biomedical papers, each of which is associated with at least one drug, target, interaction triplet. We also provide a semi-supervised dataset with 139k unique papers. We present and analyze multiple solutions, including several extractive/generative models and two data enhancement methods. The results show that the performance of those models is far from industry demand, indicating that the dataset presents a challenging research problem for the community. The dataset will be freely accessible after the review process.
B-d4pw2JdT	MAIN: A Multi-agent Indoor Navigation Benchmark for Cooperative Learning	https://openreview.net/forum?id=B-d4pw2JdT	Multi-agent, Reinforcement learning, Embodied Navigation	Previous works have proposed many multi-agent reinforcement learning methods to study this problem in diverse multi-agent environments. However, these environments have two limitations, which make them unsuitable for real-world applications: 1) the agent observes clean and formatted data from the environment instead of perceiving the noisy observation by themselves from the first-person perspective; 2) large domain gap between the environment and the real world scenarios. In this paper, we propose a Multi-Agent Indoor Navigation (MAIN) benchmark, where agents navigate to reach goals in a 3D indoor room with realistic observation inputs. In the MAIN environment, each agent observes only a small part of a room via an embodied view. Less information is shared between their observations and the observations have large variance. Therefore, the agents must learn to cooperate with each other in exploration and communication to achieve accurate and efficient navigation. We collect a large-scale and challenging dataset to research on the MAIN benchmark. We examine various multi-agent methods based on current research works on our dataset. However, we find that the performances of current MARL methods does not improve by the increase of the agent amount. We find that communication is the key to addressing this complex real-world cooperative task. By Experimenting on four variants of communication models, we show that the model with recurrent communication mechanism achieves the best performance in solving MAIN.
YJHXfcTDaqw	Rail-5k: a Real-World Dataset for Rail Surface Defects Detection	https://openreview.net/forum?id=YJHXfcTDaqw	Real-world, Object detection, Semantic segmentation, Rail surface defects	This paper presents the Rail-5k dataset for benchmarking the performance of visual algorithms in a real-world application scenario, namely the rail surface defect detection task.       We collected over 5k high-quality images from railways across China and annotated 1100 images with the help of railway experts to identify the most common 13 types of railway defects.       The dataset can be used for two settings both with unique challenges, the first is the fully-supervised setting using the 1k labeled images for training, fine-grained nature and long-tailed distribution of defect classes make it hard for visual algorithms to tackle.       The second is the semi-supervised learning setting facilitated by the 4k unlabeled images, these 4k images are uncurated containing possible image corruptions and domain shift with the labeled images, which can not be easily tackled by previous semi-supervised learning methods.       We believe our dataset could be a valuable benchmark for evaluating the robustness and reliability of visual algorithms.
vyyHxhnZJSB	GrowSpace: Learning How to Shape Plants	https://openreview.net/forum?id=vyyHxhnZJSB	Reinforcement Learning, Benchmark, Control, Hierarchical learning, Fairness, Multi-Objective Learning	Plants are dynamic systems that are integral to our existence and survival. Plants are faced with  environment changes and adapt over time to their surrounding conditions. We argue that plant responses to an environmental stimulus are a good example of a real-world problem that can be approached within a reinforcement learning (RL) framework. With the objective of controlling a plant by moving the light source, we propose GrowSpace, as a new RL benchmark. The back-end of the simulator is implemented using the Space Colonisation Algorithm, a plant growing model based on competition for space. Compared to video game RL environments, this simulator addresses a real-world problem and serves as a test bed to visualize plant growth and movement in a faster way than physical experiments. GrowSpace is composed of a suite of challenges that tackle several problems such as control, hierarchical learning, fairness and multi-objective learning. We provide an agent baselines alongside case studies to demonstrate the difficulty of the proposed benchmark.
ldUthK0EVx8	Reconstruction benchmark for obfuscated representations	https://openreview.net/forum?id=ldUthK0EVx8	privacy, inversion, inference	In this work, we tackle the question of how to benchmark reconstruction of inputs from DNN representations. This inverse problem is of great importance in the privacy community where obfuscated representation has been proposed as a technique for privacy-preserving ML inference. In this benchmark, we characterize different obfuscation techniques and identify different attack models. We propose multiple reconstruction techniques based upon different side knowledge of the adversary. Finally, we propose a benchmark that has a dataset of obfuscated representations of many existing image datasets and a framework that would allow ease of integration and benchmarking with the existing obfuscation and proposed reconstruction techniques.
70XgWMiBeH7	New Zealand Open Environmental Science Data sets	https://openreview.net/forum?id=70XgWMiBeH7	Environmental Science, New Zealand, Datasets	Data Science on environmental spatio-temporal data is becoming a critical and challenging research topic due to the changing nature and rapidly increasing volume of available data. To this end, we would like to introduce TAIAO data repository, comprising of over 30 datasets of various types including images, videos, textual and tabular data.
AcL1ORzw0Nf	Hi-Phy: A Benchmark for Hierarchical Physical Reasoning	https://openreview.net/forum?id=AcL1ORzw0Nf	Physical Reasoning Benchmark, Physical Reasoning Hierarchy, Physical Reasoning, Intelligent Agents, Angry Birds	Reasoning about the behaviour of physical objects is a key capability of agents operating in physical worlds. Humans are very experienced in physical reasoning while it remains a major challenge for AI. To facilitate research addressing this problem, several benchmarks have been proposed recently. However, these benchmarks do not enable us to measure an agent's granular physical reasoning capabilities when solving a complex reasoning task. In this paper we propose a new benchmark for physical reasoning that allows us to test individual physical reasoning capabilities. Inspired by how humans acquire these capabilities, we propose a general hierarchy of physical reasoning capabilities with increasing complexity. Our benchmark tests capabilities according to this hierarchy through generated physical reasoning tasks in the video game Angry Birds. This benchmark enables us to conduct a comprehensive agent evaluation by measuring the agent's granular physical reasoning capabilities. We conduct an evaluation with human players, learning agents, and heuristic agents and determine their capabilities. Our evaluation shows that learning agents, with good local generalization ability, still struggle to learn the underlying physical reasoning capabilities and perform worse than current state-of-the-art heuristic agents and humans. We believe that this benchmark will encourage researchers to develop intelligent agents with advanced, human-like physical reasoning capabilities.
7hQLXPnfrqk	Arena: A Scalable and Configurable Benchmark for Policy Learning	https://openreview.net/forum?id=7hQLXPnfrqk	GNN-based policy, object-based state representation, scalable policy, configurable benchmark	We believe current benchmarks for policy learning lack two important properties: scalability and configurability. The growing literature on modeling policies as graph neural networks calls for an object-based benchmark where the number of objects can be arbitrarily scaled and the mechanics can be freely configured. We introduce the Arena benchmark, a scalable and configurable benchmark for policy learning. Arena provides an object-based game-like environment where the number of objects can be arbitrarily scaled and the mechanics can be configured with a large degree of freedom. In this way, arena is designed to be an all-in-one environment that uses scaling and configuration to smoothly interpolates multiple dimensions of decision making that require different degrees of inductive bias.
TUplOmF8DsM	MQBench: Towards Reproducible and Deployable Model Quantization Benchmark	https://openreview.net/forum?id=TUplOmF8DsM	Quantization-aware Training, Post-training Quantization, Benchmark	Model quantization has emerged as an indispensable technique to accelerate deep learning inference.        Although researchers continue to push the frontier of quantization algorithms, existing quantization work is often unreproducible and undeployable.        This is because researchers do not choose consistent training pipelines and ignore the requirements for hardware deployments.        In this work, we propose Model Quantization Benchmark (MQBench), a first attempt to evaluate, analyze, and benchmark the reproducibility and deployability for model quantization algorithms.        We choose multiple different platforms for real-world deployments, including CPU, GPU, ASIC, DSP, and evaluate extensive state-of-the-art quantization algorithms under a unified training pipeline.        MQBench acts like a bridge to connect the algorithm and the hardware.        We conduct a comprehensive analysis and find considerable intuitive or counter-intuitive insights. By aligning up the training settings, we find existing algorithms have about-the-same performance on the conventional academic track. While for the hardware-deployable quantization, there is a huge accuracy gap and still a long way to go. Surprisingly, no existing algorithm wins every challenge in MQBench, and we hope this work could inspire future research directions.
GfWRFRuO51B	Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation	https://openreview.net/forum?id=GfWRFRuO51B	off-policy evaluation, real-world dataset, open-source software, benchmark experiments, offline contextual bandits	Off-policy evaluation (OPE) aims to estimate the performance of hypothetical policies using data generated by a different policy. Because of its huge potential impact, there has been growing research interest in OPE. There is, however, no real-world public dataset that enables the evaluation of OPE, making its experimental studies unrealistic and irreproducible. With the goal of enabling realistic and reproducible OPE research, we publicize Open Bandit Dataset collected on a large-scale fashion e-commerce platform, ZOZOTOWN. Our dataset is unique in that it contains a set of multiple logged bandit feedback datasets collected by running different policies on the same platform. This enables realistic and reproducible experimental comparisons of different OPE estimators for the first time. We also develop Python software called Open Bandit Pipeline to streamline and standardize the implementation of batch bandit algorithms and OPE. Our open data and pipeline will contribute to the fair and transparent OPE research and help the community identify fruitful research directions. Finally, we provide extensive benchmark experiments of existing OPE estimators using our data and pipeline. The results open up essential challenges and new avenues for future OPE research.
LL_LfK7dfpR	NATURE: Natural Auxiliary Text Utterances forRealistic Spoken Language Evaluation	https://openreview.net/forum?id=LL_LfK7dfpR	slot filling, intent detection, dialog system, natural language understanding, virtual assistant	Slot-filling and intent detection are the backbone of conversational agents such as voice assistants and they are active areas of research. Even though state-of-the-art techniques on publicly available benchmarks show impressive performance, their ability to generalize to realistic scenarios has yet to be improved. In this work, we present NATURE, a set of simple spoken language oriented transformations, applied to the evaluation set of datasets, to introduce human spoken language variations while preserving the semantics of an utterance. We apply NATURE to common slot-filling and intent detection benchmarks and demonstrate that simple deviations from the standard test set by NATURE can deteriorate model's performance significantly. Additionally, we apply different strategies to mitigate the effects of NATURE and report that data-augmentation leads to some improvement.
cIrPX-Sn5n	Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks	https://openreview.net/forum?id=cIrPX-Sn5n		Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we consistently evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase~\citep{samvelyan19smac} to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.
fYxEnpY-__G	An Empirical Study of Graph Contrastive Learning	https://openreview.net/forum?id=fYxEnpY-__G	Graph contrastive learning, self-supervised learning, graph neural networks	Graph Contrastive Learning (GCL) establishes a new paradigm for learning graph representations without human annotations. Although remarkable progress has been witnessed recently, the success behind GCL is still left somewhat mysterious. In this work, we first identify several critical design considerations within a general GCL paradigm, including augmentation functions, contrasting modes, contrastive objectives, and negative sampling techniques. Then, to understand the interplay of different GCL components, we conduct extensive, controlled experiments over a set of benchmark tasks on datasets across various domains. Our empirical studies suggest a set of general receipts for effective GCL, e.g., simple topology augmentation that produces sparse graphs brings most performance improvements; contrasting modes should be aligned with the granularities of end tasks. In addition, to foster future research and ease the implementation of GCL algorithms, we develop an easy-to-use toolbox PyGCL, featuring modularized CL components, standardized evaluation, and experiment management. We envision this work to provide useful empirical evidence of existing GCL architectures and offer several insights for future research.
CYhs2XrEFNA	Multi-Domain Conditional Image Translation: Translating Driving Datasets from Clear-Weather to Adverse Conditions	https://openreview.net/forum?id=CYhs2XrEFNA	Image Translation, Self-supervised Learning, Unsupervised Learning, Dataset Translation, Adverse-weather Translation, Generative Modeling	Vision systems for fully autonomous navigation must perform well even in unstructured and degraded scenarios. In most driving datasets today, there is a bias toward clear-weather conditions as compared with extreme-weather, owing to the difficulty in capturing and annotating large-scale image datasets degraded by extreme weather. While there has been extensive research on techniques such as deraining, dehazing and on tasks such as segmentation and domain adaptation, there has been minimal attention toward methods to effectively translate clear-weather driving datasets to extreme-weather domains. To address this, we present a method that builds on recent advances in Generative Networks and Self-Supervised Learning to perform conditional multi-domain image translation. We evaluate our method on the semantic scene understanding task and demonstrate quantitatively superior translation results from clear-weather conditions to extreme-weather shifted domains such as Rain, Night and Fog conditions. From our experiments, we show improved domain invariant content disentanglement and models trained with datasets translated with our method have improved segmentation performance over single and multi-domain image translation baselines on real-world datasets.
4Y3vzl_NEyG	CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark	https://openreview.net/forum?id=4Y3vzl_NEyG	Chinese Biomedical Language Understanding, Benchmark, Dataset	Artificial intelligence (AI), along with the recent progress in biomedical language understanding, is gradually changing medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most such benchmarks are limited to English, which has made it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11  pre-trained Chinese models, and results show that state-of-the-art neural models perform by far worse than the human ceiling. Our benchmark is released at https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us.
xZWBVuL_ip9	A Dataset for Label Aggregation from Crowdsourced Triplet Similarity Comparisons	https://openreview.net/forum?id=xZWBVuL_ip9	Crowdsourcing, Label Aggregation, Crowdsourced Triplet Similarity Comparison	Organizing objects such as human ideas, opinions, and designs based on their similarity relationships is an important first step in data exploration and decision making. Those similarity comparisons are often cast as triplet comparisons asking which of two given objects is more similar to another given object, because humans are better at this type of relative judgments than pairwise similarity comparisons which ask for absolute judgments, especially in sensory domains. Crowdsourcing is an effective way to collect such human judgments easily and on a large scale; however, there is a large variation in abilities among workers and the difficulties of evaluating the target objects. How to aggregate the labels of crowdsourced triplet similarity comparisons for estimating similarity relations of all objects when there are only a smaller number of labels remains a challenge. In this work, we construct two novel real datasets for investigating this research topic. For label aggregation approach, we propose a family of models to learn the object embeddings from crowdsourced triplet similarity comparisons by incorporating worker abilities and object difficulties. Because of the diverse properties of real datasets, we automatically search for the optimal model from all variants of the proposed model. The experimental results verified the effectiveness of our approach. We also investigated how the data properties and model options influence the performance.
9_eyi9ymJcZ	Multimodal AutoML on Tables with Text Fields	https://openreview.net/forum?id=9_eyi9ymJcZ	Multimodal AutoML, Text Data, Tabular Data, Natural Language Processing, Supervised Learning	We consider the design of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but text fields as well. Here we assemble 15 multimodal data tables that each contain some text fields and stem from a real business application. Over this benchmark, we evaluate numerous multimodal AutoML strategies, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. We identify practically superior strategies based on multimodal adaptations of Transformer networks and stack ensembling of these networks with classical tabular models. Compared with human data science teams, the best fully automated methodology discovered through our benchmark manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.
X-sH548WreZ	RobustBench: a standardized adversarial robustness benchmark	https://openreview.net/forum?id=X-sH548WreZ	Adversarial robustness, Adversarial examples, Benchmarking robustness, Deep learning	As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a potential solution, they have to be highly customized for particular models, which makes it difficult to compare different methods. Our goal is to instead establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To evaluate robustness of models for our benchmark, we consider AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. We also impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. Our leaderboard, hosted at http://robustbench.github.io/, contains evaluations of 90+ models and aims at reflecting the current state of the art on a set of well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library http://github.com/RobustBench/robustbench that provides unified access to 60+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.
yJyIjWyPJgs	Towards a robust experimental framework and benchmark for lifelong language learning	https://openreview.net/forum?id=yJyIjWyPJgs	lifelong learning, continual learning, natural language processing	In lifelong learning, a model learns different tasks sequentially throughout its lifetime. State-of-the-art deep learning models, however, struggle to generalize in this setting and suffer from catastrophic forgetting of old tasks when learning new ones. While a number of approaches have been developed in an attempt to ameliorate this problem, there are no established, unified or generalized frameworks for rigorous evaluations of proposed solutions; a problem which is particularly pronounced in the domain of NLP. The few existing benchmarks are typically limited to a specific flavor of lifelong learning -- continual open set classification -- where new classes, as opposed to tasks, are learned incrementally. Moreover, the only general lifelong learning benchmark combines a multi-label classification setup with a multi-class classification setup resulting in misleading gradients during training. We empirically demonstrate that the catastrophic forgetting observed here can be attributed to the experimental design rather than to any inherent modeling limitations. To address these issues, we propose an experimental framework for true, general lifelong learning in NLP. Using this framework, we develop a comprehensive suite of benchmarks that target different properties of lifelong learning (e.g., forgetting or intransigence); experiment with diverse facets of language learning: multi-domain, multilingual and different levels of linguistic hierarchy; and present a continuous evaluation scheme under a new metric: Area Under the Lifelong Test Curve. Our framework reveals shortcomings of prevalent memory-based solutions, demonstrating they are unable to outperform a simple experience replay baseline under the realistic lifelong learning setup.
aEMLMjbvftN	On Predicting and Generating a Good Break Shot in Billiards Sports	https://openreview.net/forum?id=aEMLMjbvftN	billiards sports, prediction, generation	With the proliferation of tracking devices such as cameras and/or GPS sensors, sports data is being generated at an unprecedented speed and the interest in collecting some data from sports games has grown dramatically as well. The collected data facilitates various sports analytic tasks; however, these studies are mainly concerning about sports such as football and basketball. It remains largely unexplored for billiards sports though it is a popular sport of both strategy and physical skill, which is mainly due to the lack of publicly available datasets. To this end, we collect a dataset of billiards sports, which consists of billiards layouts (i.e., locations) of billiards balls after performing break shots, called break shot layouts. To support various analytic tasks, we also collect many rich remarks and labels that are associated with each layout. Our dataset includes around 3000 international professional break shot layouts in the most recent two decades.  On top of the dataset, we investigate two machine learning tasks regarding prediction and generation, which are fundamental in billiards sports and can serve many applications. We conduct extensive experiments on the collected dataset for the two tasks, and the results demonstrate the superior performance we achieve. The quality of the collected dataset has also been verified.
h-flVCIlstW	FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information	https://openreview.net/forum?id=h-flVCIlstW	fact extraction and verification, structured information, unstructured and structured information, fact checking, natural language processing, information retrieval	Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.
VjJxBi1p9zh	RedCaps: Web-curated image-text data created by the people, for the people	https://openreview.net/forum?id=VjJxBi1p9zh		Datasets of images and text have become increasingly popular for learning representations that generalize to visual recognition and vision and language tasks. Prior public datasets were built by querying search engines or collecting HTML alt-text, which require complex filtering pipelines to compensate for their noisy raw input data. We aim to collect image-text data with minimal filtering by exploring new data sources. We introduce RedCaps -- a large-scale dataset of 11.7M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks.
-wVVl_UPr8	The PAIR-R24M Dataset for Multi-animal 3D Pose Estimation	https://openreview.net/forum?id=-wVVl_UPr8	Animal Behavior, Pose Estimation, Multi-agent, Dataset, Motion Capture	Understanding the biological basis of social and collective behaviors in animals is a key goal of the life sciences, and may yield important insights for engineering intelligent multi-agent systems. A critical  step in understanding the mechanisms underlying social behaviors is a precise readout of the full 3D pose of interacting animals. While approaches for multi-animal pose estimation are beginning to emerge, they remain challenging to compare due to the lack of standardized benchmark datasets for multi-animal 3D pose estimation. Here we introduce the PAIR-R24M (Paired Acquisition of Interacting Rats) dataset for multi-animal 3D pose estimation, which contains 21.5 million frames of RGB video and 3D ground-truth motion capture of dyadic interactions in laboratory rats. PAIR-R24M contains data from 18 distinct pairs of rats across diverse behaviors, from 30 different viewpoints. The data are temporally contiguous and annotated with 11 behavioral categories, and 3 interaction behavioral categories, using a multi-animal extension of a recently developed behavioral segmentation approach. We used a novel multi-animal version of the recently published DANNCE network to establish a strong baseline for multi-animal 3D pose estimation without motion capture. These recordings are of sufficient resolution to allow us to examine cross-pair differences in social interactions, and identify different conserved patterns of social interaction across rats.
0OyT3oVv_Rk	Design-Bench: Benchmarks for Data-Driven Offline Model-Based Optimization	https://openreview.net/forum?id=0OyT3oVv_Rk	model-based optimization, benchmark tasks, offline optimization	Black-box model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function,  are ubiquitous in a wide range of domains, such as the design of proteins, DNA sequences, aircraft, and robots.  Solving model-based optimization problems typically requires actively querying the unknown objective function on design proposals, which means physically building the candidate molecule, aircraft, or robot, testing it, and storing the result. This process can be expensive and time-consuming, and one might instead prefer to optimize for the best design using only the data one already has. This setting -- called offline MBO -- poses substantial and different algorithmic challenges than more commonly studied online techniques. A number of recent works have demonstrated success with offline MBO for high-dimensional optimization problems using high-capacity deep neural networks. However, the lack of standardized benchmarks in this emerging field is making progress difficult to track. To address this, we present Design-Bench, a benchmark for offline MBO with a unified evaluation protocol and reference implementations of recent methods. Our benchmark includes a suite of diverse and realistic tasks derived from real-world optimization problems in biology, materials science, and robotics that present distinct challenges for offline MBO. Our benchmark and reference implementations are publicly available at: https://github.com/brandontrabucco/design-bench
3ZQqjt_Q6b	EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation	https://openreview.net/forum?id=3ZQqjt_Q6b	dataset, knowledge graph, NLG, graph-to-text	We introduce EventNarrative, a knowledge graph-to-text dataset from publicly available open-world knowledge graphs. Given the recent advances in event-driven Information Extraction (IE), and that prior research on graph-to-text only focused on entity-driven KGs, this paper focuses on event-centric data. However, our data generation system can still be adapted to other other types of KG data. Existing large-scale datasets in the graph-to-text area are non-parallel, meaning there is a large disconnect between the KGs and text. The datasets that have a paired KG and text, are small scale and manually generated or generated without a rich ontology, making the corresponding graphs sparse. Furthermore, these datasets contain many unlinked entities between their KG and text pairs. EventNarrative consists of approximately 230,000 graphs and their corresponding natural language text, 6 times larger than the current largest parallel dataset. It makes use of a rich ontology, all of the KGs entities are linked to the text, and our manual annotations confirm a high data quality. Our aim is two-fold: help break new ground in event-centric research where data is lacking, and to give researchers a well-defined, large-scale dataset in order to better evaluate existing and future knowledge graph-to-text models. We also evaluate two types of baseline on EventNarrative: a graph-to-text specific model and two state-of-the-art language models, which previous work has shown to be adaptable to the knowledge graph-to-text domain.
48CBzhshpcS	Irregular Shape Instance Segmentation	https://openreview.net/forum?id=48CBzhshpcS	instance segmentation, dataset	In this paper, we introduce a brand new dataset to promote the study of instance segmentation for objects with irregular shapes. Our key observation is that though irregularly shaped objects widely exist in daily life and industrial scenarios, they received little attention in the instance segmentation field due to the lack of corresponding datasets. To fill this gap, we propose iShape, an irregular shape dataset for instance segmentation. Unlike most existing instance segmentation datasets of regular objects, iShape has many characteristics that challenge existing instance segmentation algorithms, such as large overlaps between bounding boxes of instances, extreme aspect ratios, and large numbers of connected components per instance. We benchmark popular instance segmentation methods on iShape and find their performance drop dramatically. Hence, we propose an affinity-based instance segmentation algorithm, called ASIS, as a stronger baseline. ASIS explicitly combines perception and reasoning to solve Arbitrary Shape Instance Segmentation including irregular objects. Experimental results show that ASIS outperforms the state-of-the-art on iShape.
cB3OdLInAr9	AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry	https://openreview.net/forum?id=cB3OdLInAr9	tableQA, zero-shot, question-answering, dataset, benchmark, complex-tables	Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre-trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain. To address this problem, we introduce the domain-specific Table QA dataset AITQA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings (SEC Filings publicly available at: https://www.sec.gov/edgar.shtml) of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to-end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8% (RCI). We also present pragmatic table pre-processing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.
nlJ1rV6G_Iq	CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer	https://openreview.net/forum?id=nlJ1rV6G_Iq	Mammography dataset, ordinal classification, breast cancer	Interval and large invasive breast cancers, which are associated with worse prognosis than other cancers, are usually detected at a late stage due to false negative assessments of screening mammograms. The missed screening-time detection is commonly caused by the tumor being obscured by its surrounding breast tissues, a phenomenon called masking. To study and benchmark mammographic masking of cancer, in this work we introduce CSAW-M, the largest public mammographic dataset, collected from over 10,000 individuals and annotated with potential masking. In contrast to the previous approaches which measure breast image density as a proxy, our dataset directly provides annotations of masking potential assessments from five specialists. We also trained deep learning models on CSAW-M to estimate the masking level. and showed that the estimated masking is significantly more predictive of women diagnosed with interval and large invasive cancers -- without being explicitly trained for these tasks -- than its breast density counterparts.
6lYwZ6yknRw	Towards a universal dataset and metrics for training and evaluating table extraction models	https://openreview.net/forum?id=6lYwZ6yknRw	table detection, table structure recognition, table extraction, functional analysis, object detection	Recently, interest has grown in applying machine learning approaches to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging not only to make but to measure, due to several issues that arise in both training and evaluating such systems from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for a given input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these we propose a new dataset, PubMed Tables One Million (PubTables1M), and a new class of metric, grid table similarity (GriTS). PubTables1M is nearly twice as large as the current largest comparable dataset, can be used for models across multiple architectures and modalities, and addresses issues such as ambiguity and lack of consistency in the annotations. We apply DETR to table extraction for the first time and show that object detection models trained on images and bounding boxes derived from this data produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. In addition to releasing the data, we describe the dataset creation process in detail to enable others to build on our work and to ensure forward and backward compatibility of this data for combining it with other datasets created for these tasks. It is our hope that this data and the proposed metrics can further progress in this area by serving as a single source of data for training and evaluation of a wide variety of models for table extraction.
rCRyg1-Yovi	Three million images and morphological profiles of cells treated with matched chemical and genetic perturbations	https://openreview.net/forum?id=rCRyg1-Yovi	Image-based profiling, chemical perturbations, genetic perturbations, CRISPR, ORF, target identification, target deconvolution, representation learning	We present a new, carefully designed and well-annotated dataset of images and image-based profiles of cells that have been treated with chemical compounds and genetic perturbations. Each gene that is perturbed is a known target of at least two compounds in the dataset. The dataset can thus serve as a benchmark to evaluate methods for predicting similarities between compounds and between genes and compounds, measuring the effect size of a perturbation, developing style-transfer methods to predict one experimental condition from another, and more generally, learning effective representations for measuring cellular state from microscopy images.
SnC9rUeqiqd	HiRID-ICU-Benchmark --- A Comprehensive Machine Learning Benchmark on High-resolution ICU Data.	https://openreview.net/forum?id=SnC9rUeqiqd	Benchmark, Intensive Care Unit, Time Series, Patient Monitoring	The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a  benchmark covering a large spectrum of ICU-related tasks. Using the HiRID-I dataset, we define multiple clinically relevant tasks developed in collaboration with clinicians.  In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work.
aAMgwCmP930	FHIST: A Benchmark for Few-shot Classification of Histological Images	https://openreview.net/forum?id=aAMgwCmP930	few-shot, histology classification, benchmark, domain shift	Few-shot learning has recently attracted wide interest in image classification, but almost all the current standard and public benchmarks are focused on natural images. The few-shot paradigm is highly relevant in medical-imaging applications due to the scarcity of labeled data, as annotations are expensive and require specialized expertise. However, in medical imaging, few-shot learning research is sparse, limited to private data sets and is at its early stage. In particular, the few-shot setting is of high clinical interest in histology due to the high diversity and fine granularity of cancer related tissue classification tasks, and the variety of data-preparation techniques, which results in covariate shifts in the inputs and dis-parities in the labels. This paper introduces a highly diversified public benchmark, gathered from various public datasets, for few-shot histology data classification. Our benchmark builds few-shot tasks and base-training data with various tissue types, different levels of domain shifts stemming from different cancer sites, and different class granularity levels, thereby reflecting realistic clinical settings. We evaluate the performances of state-of-the-art few-shot learning methods, initially designed for natural images, on our histology benchmark. We further examine three scenarios based on the degree of domain shift between the source and the target histology datasets: (i) near-domain, (ii) middle-domain and (iii) out-domain. Our experiments reveal that the well-established and sophisticated meta-learning methods are not the best solution for this new benchmark. We observe that simple fine-tuning and regularization methods at inference, based on a standard and simple base training, achieve significantly better results than the popular episodic-training paradigm. Our code and few-shot tasks are provided along with the submission, and will be made freely available upon publication of the work.
8HkbjXqbAnz	Geon3D: Benchmarking 3D Shape Bias towards Building Robust Machine Vision	https://openreview.net/forum?id=8HkbjXqbAnz	robust vision, robustness, adversarial examples, common corruptions, 3D reconstruction, vision science	Human vision, unlike existing machine vision systems, is surprisingly robust to environmental variation, including both naturally occuring disturbances (e.g., fog, snow, occlusion) and artificial corruptions (e.g., adversarial examples). Such robustness, at least in part, arises from our ability to infer 3D geometry from 2D retinal projections---the ability to go from images to their underlying causes, including the 3D scene. How can we design machine learning systems with such strong shape bias? In this work, we view 3D reconstruction as a pretraining method for building more robust vision systems. Recent studies explore the role of shape bias in the robustness of vision models. However, most current approaches to increase shape bias based on ImageNet take an indirect approach, attempting to instead reduce texture bias via structured data augmentation. These approaches do not directly nor fully exploit the relationship between 2D features and their underlying 3D shapes. To fill this gap, we introduce a novel dataset called Geon3D, which is derived from objects that emphasize variation across shape features that the human visual system is thought to be particularly sensitive. This dataset enables, for the first time, a controlled setting where we can isolate the effect of ``3D shape bias'' in robustifying neural networks, and informs more direct approaches to increase shape bias by exploiting 3D vision tasks. Using Geon3D, we find that CNNs pretrained on 3D reconstruction are more resilient to viewpoint change, rotation, and shift than regular CNNs. Further, when combined with adversarial training, 3D reconstruction pretrained models improve adversarial and common corruption robustness over vanilla adversarially-trained models. This suggests that incorporating 3D shape bias is a promising direction for building robust machine vision systems.
NqErRNg_8Fb	SMART-Rain: A Degradation Evaluation Dataset for Autonomous Driving in Rain	https://openreview.net/forum?id=NqErRNg_8Fb	degradation evaluation, autonomous driving dataset, rainy weather	Autonomous driving in adverse weather is challenging. There is limited literature that studies degradation for autonomous driving systems in adverse weather. Two fundamental questions are often overlooked: how much degradation exists and whether such degradation exceeds the acceptable range. This paper focuses on degradation evaluation in rainy weather. In this work, we firstly propose a framework that evaluates degradation on weather conditions, sensor performance, and perception system performance. Then we present a dataset consisting of 3D LiDAR scans and camera images mainly collected in rainy weather. After that, we propose three degradation evaluation tasks corresponding to the degradation evaluation framework, including rainfall intensity estimation, LiDAR degradation estimation, 2D object detection in various rainy conditions. Finally, we present our baseline results evaluated on our dataset. The dataset, development kit, and codes are available at http://rain.smart.mit.edu/smartrain/
5Str2l1vmr-	The Benchmark Lottery	https://openreview.net/forum?id=5Str2l1vmr-	Benchmark Lottery, Benchmarking, Machine Learning Evaluation	The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of a benchmark lottery that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems and reinforcement learning.
fe_hCc4RBrg	Programming Puzzles	https://openreview.net/forum?id=fe_hCc4RBrg	programming puzzles, program synthesis, language models, GPT-3, Python, coding, problems, dataset	We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input $x$ which makes $f$ output "True". The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f(x)$ is all that is needed to test a candidate solution $x$.  They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping. We develop baseline enumerative program synthesis and GPT-3 solvers that are capable of solving easy puzzles---even without access to any reference solutions---by learning from their own past solutions. Based on a small user study, we find puzzle difficulty to correlate between human programmers and the baseline AI solvers.
tNMPcy9PovC	Measuring the State of Document Understanding	https://openreview.net/forum?id=tNMPcy9PovC	Document Understanding, Multi-modal Models, Language Models, NLP, Multimodal Data, Key Information Extraction, Question Answering, Information Extraction, Table Comprehension, KIE, NLI, Visual QA, Layout-aware Language Models	Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community.       Additionally, the lack of a commonly accepted benchmark made it difficult to quantify progress in the domain. To empower research in Document Understanding, we present a suite of tasks that fulfill the highest quality, difficulty, and licensing criteria. The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains, and layouts featuring tables, graphs, lists, and infographics. The current study reports systematic baselines making use of recent advances in layout-aware language modeling. To support adoption by other researchers, both the benchmarks and reference implementations will be shortly released.
saUHJ1iZhiO	TWEET-FD: A Dataset for Multiple Foodborne Illness Incident Detection Tasks	https://openreview.net/forum?id=saUHJ1iZhiO	foodborne illness detection, social media, transformer-based model, multi-task learning	Foodborne illnesses are a serious but preventable public health problem with delays in detecting these outbreaks resulting in productivity loss, expensive recalls, public safety hazards,  and even loss of life. While social media are a promising source for identifying unreported foodborne illnesses early, there is a dearth of relevant labeled datasets available to the community for developing effective detection models. To accelerate the development of machine learning-based models for foodborne illness detection, we thus present TWEET-FD (TWEET-Foodborne illness Detection), the first publicly available dataset for multiple foodborne illness incident detection tasks. TWEET-FD collected from Twitter is annotated with three facets: (i) Tweet Class: tweet classification as foodborne illness incident, (ii) Entity Type: entities of interest for foodborne illness detection in the tweet, and (iii) Entity Relevance: relevancy of entities to the foodborne illness incident. We then introduce several domain tasks leveraging these three facets: text relevance classification (TRC), entity mention detection (EMD), and entity relevance classification (ERC). Additionally, we derive the relevant entity detection (RED) task by combining entity type and entity relevance facets. We describe the end-to-end methodology for dataset design, creation, and labeling for supporting model development for these tasks. We provide baseline results for these tasks leveraging state-of-the-art  NLP-based deep learning methods. This dataset opens opportunities for promising future research in foodborne illness incident detection and resolution.
pBwQ82pYha	Graph Robustness Benchmark: Rethinking and Benchmarking Adversarial Robustness of Graph Neural Networks	https://openreview.net/forum?id=pBwQ82pYha	adversarial robutness, attack and defense, adversarial learning, graph neural networks	Recent studies have shown that Graph Neural Networks (GNNs) are vulnerable to adversarial attacks. Previous attacks and defenses on GNNs face common problems like scalability or generality, which hinder the progress of this domain. By rethinking limitations in previous works, we propose Graph Robustness Benchmark (GRB), the first benchmark that aims to provide scalable, general, unified, and reproducible evaluation on adversarial robustness of GNNs. GRB includes (1) scalable datasets processed by a novel splitting scheme; (2) diverse set of baseline methods covering GNNs, attacks, and defenses; (3) unified evaluation pipeline that permits a fair comparison; (4) modular coding framework that facilitates implementation of various methods and ensures reproducibility; (5) leaderboards that track the progress of the field. Besides, we propose two strong baseline defenses that significantly outperform previous ones. With extensive experiments, we can fairly compare all methods and investigate their pros and cons. GRB is open-source and maintains all datasets, codes, leaderboards at https://cogdl.ai/grb/home, which will be continuously updated to promote future research in this field.
lg3iGvFQ5V	Benchmarking the Robustness of CNN-based Spatial-Temporal Models	https://openreview.net/forum?id=lg3iGvFQ5V		The state-of-the-art deep convolutional neural networks are vulnerable to common corruptions in nature (e.g., input data corruptions caused by weather changes, system errors). While rapid progress has been made in analyzing and improving the robustness of models in image understanding, the robustness in video understanding is largely ignored. In this paper, we establish a corruption robustness benchmark, Mini Kinetics-C and Mini SSV2-C, which considers temporal corruptions beyond spatial corruptions in images. We make the first attempt to conduct an exhaustive study on corruption robustness in terms of spatial and temporal domain, using established CNN-based spatial-temporal models. The study provides some guidance on robust model design, training and inference: 1) 3D modules make video classification models more robust instead of 2D modules, 2) longer input length and uniform sampling of input frames can benefit model corruption robustness, 3) model corruption robustness (especially robustness in the temporal domain) enhances with computational cost, which may contradict with the current trend of improving the computational efficiency of models. Our codes are available on https://github.com/Newbeeyoung/Video-Corruption-Robustness.
ps95-mkHF_	B-Pref: Benchmarking Preference-Based Reinforcement Learning	https://openreview.net/forum?id=ps95-mkHF_	Preference-based reinforcement learning, human-in-the-loop reinforcement learning, deep reinforcement learning	Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically.
xiFJn0fbFX9	The Medkit-Learn(ing) Environment: Medical Decision Modelling through Simulation	https://openreview.net/forum?id=xiFJn0fbFX9	Decision Modelling, Imitation Learning, Inverse Reinforcement Learning, Simulation, Benchmark	Understanding decision-making in clinical environments is of paramount importance if we are to bring the strengths of machine learning to ultimately improve patient outcomes. Several factors including the availability of public data, the intrinsically offline nature of the problem, and the complexity of human decision making, has meant that the mainstream development of algorithms is often geared towards optimal performance in tasks that do not necessarily translate well into the medical regime; often overlooking more niche issues commonly associated with the area. We therefore present a new benchmarking suite designed specifically for medical sequential decision making: the Medkit-Learn(ing) Environment, a publicly available Python package providing simple and easy access to high-fidelity synthetic medical data. While providing a standardised way to compare algorithms in a realistic medical setting we employ a generating process that disentangles the policy and environment dynamics to allow for a range of customisations, thus enabling systematic evaluation of algorithms' robustness against specific challenges prevalent in healthcare.
f5zF1mgCLR0	A2X: An Agent and Environment Interaction Benchmark for Multimodal Human Trajectory Prediction	https://openreview.net/forum?id=f5zF1mgCLR0	human trajectory prediction, datasets, evaluation metrics	Recent trends in human trajectory prediction are the development of generative models which generate distributions of trajectories. However existing metrics are suited only for single (unimodal) trajectory instances. Furthermore, existing datasets are largely limited to small-scale interactions between people, with little to no agent-to-agent environment interaction. To address these challenges, we propose a dataset that compensates for the lack of agent-to-environment interaction in existing datasets with a new simulated dataset and metrics to convey model performance with more reliability and nuance. A subset of these metrics are novel multiverse metrics, which are better-suited for multimodal models than existing metrics but are still applicable to unimodal models. Our results showcase the benefits of the augmented dataset and metrics. The dataset is available at: https://mubbasir.github.io/HTP-benchmark/.
ktkr1GgZrK4	Multi-Dataset Benchmarks for Masked Identification using Contrastive Representation Learning	https://openreview.net/forum?id=ktkr1GgZrK4	representation learning, masked recognition, benchmarks, siamese networks, one-shot learning	The COVID-19 pandemic has drastically changed accepted norms globally. Within the past year, masks have been used as a public health response to limit the spread of the virus. This sudden change has rendered many face recognition based access control, authentication and surveillance systems ineffective.        Official documents such as passports, driving license and national identity cards are enrolled with fully uncovered face images. However, in the current global situation, face matching systems should be able to match these reference images with masked face images. As an example, in an airport or security checkpoint it is safer to match the unmasked image of the identifying document to the masked person rather than asking them to remove the mask. We find that current facial recognition techniques are not robust to this form of occlusion.              To address this unique requirement presented due to the current circumstance, we propose a set of re-purposed datasets and a benchmark for researchers to use. We also propose a contrastive visual representation learning based pre-training workflow which is specialized to masked vs unmasked face matching. We ensure that our method learns robust features to differentiate people across varying data collection scenarios. We achieve this by training over many different datasets and validating our result by testing on various holdout datasets.              The specialized weights trained by our method outperform standard face recognition features for masked to unmasked face matching. We believe the provided synthetic mask generating code, our novel training approach and the trained weights from the masked face models will help in adopting existing face recognition systems to operate in the current global environment. We open-source all contributions for broader use by the research community.
OTnqQUEwPKu	Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics	https://openreview.net/forum?id=OTnqQUEwPKu	fairness model evaluation, fair deep learning, adversarial fairness	With the recent expanding attention of machine learning researchers and practitioners to fairness, there is a void of a common framework to analyze and compare the capabilities of proposed models in deep representation learning. In this paper, we evaluate different fairness methods trained with deep neural networks on a common synthetic dataset and a real-world dataset to obtain a better insight into the working of these methods. In particular, we train about 3000 different models in various setups, including imbalanced and correlated data configurations, to verify the limits of the current models and better understand in which setups they are subject to failure. Our results show that the bias of models increase as datasets become more imbalanced or datasets attributes become more correlated, the level of dominance of correlated sensitive dataset features impact bias, and the sensitive information remains in the latent representation even when bias-mitigation algorithms are applied. Overall, we present a dataset, propose various challenging evaluation setups, and rigorously evaluate recent promising bias-mitigation algorithms in a common framework and publicly release this benchmark, hoping the research community would take it as a common entry point for fair deep learning.
Uk2mymgn_LZ	DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning	https://openreview.net/forum?id=Uk2mymgn_LZ	domain agnostic, self-supervised learning, benchmark, representation learning, unsupervised learning, transfer learning	Self-supervised learning algorithms, including BERT and SimCLR, have enabled significant strides in fields like natural language processing, computer vision, and speech processing. However, the domain-specificity of these algorithms means that solutions must be handcrafted for each new setting, including myriad healthcare, scientific, and multimodal domains. To catalyze progress towards more domain-agnostic methods, we introduce DABS: a Domain-Agnostic Benchmark for Self-supervised learning. To perform well on DABS, an algorithm must be pretrained on six unlabeled datasets from diverse domains: natural images, text, speech recordings, medical imaging, multichannel sensor data, and paired text and images, and then perform well on a set of labeled tasks in each domain. We also present e-Mix and ShED: two baseline domain-agnostic algorithms; their modest performance demonstrates that significant progress is needed before self-supervised learning is an out-of-the-box solution for arbitrary domains. Code for benchmark datasets and baseline algorithms is available at [redacted].
99P5Hbti02	HPO-B: a Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML	https://openreview.net/forum?id=99P5Hbti02	Meta-dataset, Hyperparameter Optimization, OpenML, Transfer-learning, Meta-learning	Hyperparameter optimization (HPO) is a core problem for the machine learning community and remains largely unsolved due to the significant computational resources required to evaluate hyperparameter configurations. As a result, a series of recent related works have focused on the direction of transfer learning for quickly fine-tuning hyperparameters on a dataset. Unfortunately, the community does not have a common large-scale benchmark for comparing HPO algorithms. Instead, the de facto practice consists of empirical protocols on arbitrary small-scale meta-datasets that vary inconsistently across publications, making reproducibility a challenge. To resolve this major bottleneck and enable a fair and fast comparison of black-box HPO methods on a level playing field, we propose HPO-B, a new large-scale benchmark in the form of a collection of meta-datasets. Our benchmark is assembled and preprocessed from the OpenML repository and consists of 176 search spaces (algorithms) evaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter evaluations. For ensuring reproducibility on our benchmark, we detail explicit experimental protocols, splits, and evaluation measures for comparing methods for both non-transfer, as well as, transfer learning HPO.
aH0NedstEei	Pl@ntNet-300K: a new plant image dataset for the evaluation of set-valued classifiers	https://openreview.net/forum?id=aH0NedstEei	dataset, ambiguity, top-k, set-valued classification, long tail, plant	This paper presents a novel image dataset with high intrinsic ambiguity specifically built for evaluating and comparing set-valued classifiers. This dataset, built from the database of Pl@ntnet citizen observatory, consists of 306,146 images covering 1,081 species. We highlight two particular features of the dataset, inherent to the way the images are acquired and to the intrinsic diversity of plants morphology:           i) The dataset has a strong class imbalance, meaning that a few species account for most of the images.           ii) Many species are visually similar, making identification difficult even for the expert eye.       These two characteristics make the present dataset well suited for the evaluation of set-valued classification methods and algorithms. Therefore, we recommend two set-valued evaluation metrics associated with the dataset (top-k and average-k) and we provide the results of a baseline approach based on a resnet50 trained with the cross-entropy loss.
bKBhQhPeKaF	Benchmark for Compositional Text-to-Image Synthesis	https://openreview.net/forum?id=bKBhQhPeKaF	text-to-image synthesis, text-to-image generation, compositionality	Rapid progress in text-to-image generation has been often measured by Frechet Inception Distance (FID) to capture how realistic the generated images are, or by R-Precision to assess if they are well conditioned on the given textual descriptions. However, a systematic study on how well the text-to-image synthesis models generalize to novel word compositions is missing. In this work, we focus on assessing how true the generated images are to the input texts in this particularly challenging scenario of novel compositions. We present the first systematic study of text-to-image generation on zero-shot compositional splits targeting two scenarios, unseen object-color (e.g. "blue petal") and object-shape (e.g. "long beak") phrases. We create new benchmarks building on the existing CUB and Oxford Flowers datasets. We also propose a new metric, based on a powerful vision-and-language CLIP model, which we leverage to compute R-Precision. This is in contrast to the common approach where the same retrieval model is used during training and evaluation, potentially leading to biased behavior. We experiment with several recent text-to-image generation methods. Our automatic and human evaluation confirm that there is indeed a gap in performance when encountering previously unseen phrases. We show that the image correctness rather than purely perceptual quality is especially impacted. Finally, our CLIP-R-Precision metric demonstrates better correlation with human judgments than the commonly used metric.
ooWbKrRIk9G	The Argoverse Trajectory Retrieval Benchmark	https://openreview.net/forum?id=ooWbKrRIk9G	information retrieval, autonomous vehicles, trajectory embeddings	As tracking data becomes more readily available in many domains such as sports, animal tracking, and autonomous vehicles, so does the need for effective information access and retrieval of those growing datasets. To that end, we develop the Argoverse Trajectory Retrieval Benchmark for contextual trajectory retrieval of driving scenarios.  The goal of this task is to find similar trajectories from within a large dataset given a query trajectory. This task is challenging because there are many dimensions of variation in which two trajectories can be similar, such as vehicle kinematics, social causality, and road configurations. To our knowledge, this is the first standardized benchmark for trajectory retrieval of driving scenarios. We also provide an evaluation of baseline approaches based on representation learning and relevance feedback, and highlight several areas for improvement for which machine learning can play a large role in future work.
dtjnxfS9OE4	Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments	https://openreview.net/forum?id=dtjnxfS9OE4	reinforment learning	The capability of reinforcement learning (RL) agent directly depends on the diversity of learning scenarios the environment generates and how closely it captures real-world situations. However, existing environments/simulators lack the support to systematically model distributions over initial states and transition dynamics.  Furthermore, in complex domains such as soccer, the space of possible scenarios is infinite, which makes it impossible for one research group to provide a comprehensive set of scenarios to train, test, and benchmark RL algorithms. To address this issue, for the first time, we adopt an existing formal scenario specification language, SCENIC, to intuitively model and generate interactive scenarios. We interfaced SCENIC to Google Research Soccer environment to create a platform called SCENIC4RL. Using this platform, we provide a dataset consisting of 36 scenario programs encoded in SCENIC and demonstration data generated from a subset of them. We share our experimental results to show the effectiveness of our dataset and the platform to train, test, and benchmark RL algorithms. More importantly, we open-source our platform to enable RL community to collectively contribute to constructing a comprehensive set of scenarios.
XccDXrDNLek	Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks	https://openreview.net/forum?id=XccDXrDNLek	label errors, datasets, benchmarks, data-centric, confident learning, noisy labels, dataset curation	We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.3% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.
pOf_IV_-XG	Using Dynamic Neural Networks to Model the Speed-Accuracy Trade-Off in People	https://openreview.net/forum?id=pOf_IV_-XG	visual object recognition, computer vision, psychophysics, psychology	Neural networks have been shown to exhibit remarkable object recognition performance. We ask here whether such networks can provide a useful model for how people recognize objects. Human recognition time varies, from 0.1 to 10 s, depending on the stimulus and task. Slowness of recognition is a key feature in some public health issues, such as dyslexia, so it is crucial to create a model of human speed-accuracy trade-offs. This is an essential aspect of any useful computational model of human cognitive behavior. We present a benchmark dataset for human speed-accuracy trade-off in recognizing a CIFAR-10 image~\cite{Krizhevsky09learningmultiple} from a set of provided class labels. Within a series of trials, a beep sounds at a fixed delay after the target (the desired reaction time), and the response counts only if it occurs near that time. We observe that accuracy grows with reaction time and examine several dynamic neural networks that exhibit a speed-accuracy trade-off as humans do. After limiting the network resources and adding image perturbations (grayscale conversion, noise, blur) to bring the two observers (human and network) into the same accuracy range, humans and networks show very similar dependence on duration or floating point operations (FLOPS). We conclude that dynamic neural networks are a promising model of human reaction time in recognition tasks. Understanding how the brain allocates appropriate resources under time pressure would be a milestone in neuroscience and a first step toward understanding conditions like dyslexia. Our dataset and code are publicly available.
JH61CD7afTv	LiRo: Benchmark and leaderboard for Romanian language tasks	https://openreview.net/forum?id=JH61CD7afTv	benchmark, NLP, Romanian language, debiasing	Recent advances in NLP have been sustained by the availability of large amounts of data and standardized benchmarks, which are not available for many languages. As a small step towards addressing this we propose LiRo, a platform for benchmarking models on the Romanian language on nine standard tasks: text classification, named entity recognition, machine translation, sentiment analysis, POS tagging, dependency parsing, language modelling, question-answering, and semantic textual similarity. We also include a less standard task of embedding debiasing, to address the growing concerns related to gender bias in language models. The platform exposes per-task leaderboards populated with baseline results for each task. In addition, we create three new datasets: one from Romanian Wikipedia and two by translating the Semantic Textual Similarity (STS) benchmark and the Cross-lingual Question Answering Dataset (XQuAD) into Romanian. We believe LiRo will not only add to the growing body of benchmarks covering various languages, but can also enable multi-lingual research by augmenting parallel corpora, and hence is of interest for the wider NLP community. LiRo is available at https://lirobenchmark.github.io/.
aIfp8kLuvc9	TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers	https://openreview.net/forum?id=aIfp8kLuvc9	Tensor Programs, Compiler, Code optimization, Cost Model, Auto-tuning	Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.              We introduce TenSet, a large-scale tensor program performance dataset. TenSet contains 52 million program performance records collected from 6 hardware platforms. We provide comprehensive studies on how to learn and evaluate the cost models, including data collection, model architectures, loss functions, transfer learning, and evaluation metrics. We also show that a cost model pre-trained on TenSet can accelerate the search time in the state-of-the-art tensor compiler by up to 10$\times$. The dataset is available at https://github.com/tlc-pack/tenset.
qF7FlUT5dxa	CommonsenseQA 2.0: Exposing the Limits of AI through Gamification	https://openreview.net/forum?id=qF7FlUT5dxa	NLP, Question Answering, Dataset, Common sense, Gamification	Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction.        The goal of players in the game is to compose questions that mislead a rival AI while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself.       Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3 (52.9%) in a few-shot inference setup.  Both score well below human performance which is at 94.1%.
IsK8iKbL-I	The Caltech Off-Policy Policy Evaluation Benchmarking Suite	https://openreview.net/forum?id=IsK8iKbL-I	reinforcement learning, off-policy evaluation, benchmark, OPE, RL, off-policy policy evaluation, empirical study	We offer an experimental benchmark for off-policy policy evaluation (OPE) in reinforcement learning, which is a key problem in many safety critical applications. Given the increasing interest in deploying learning-based methods, there has been a flurry of recent proposals for OPE method, leading to a need for standardized empirical analyses.  Our work takes a strong focus on diversity of experimental design to enable stress testing of OPE methods.  We provide a comprehensive benchmarking suite to study the interplay of different attributes on method performance. We also show how to distill the results into a summarized set of guidelines for OPE in practice. Our software package is open-sourced and we invite interested researchers to further contribute to the benchmark.
9E3dTIMxL8S	VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation	https://openreview.net/forum?id=9E3dTIMxL8S	video-and-language, benchmark, multi-task	Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE)  benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at https://value-leaderboard.github.io/.
oRHIGl9PMxj	Synthetic Benchmarks for Scientific Research in Explainable Machine Learning	https://openreview.net/forum?id=oRHIGl9PMxj	explainability, synthetic data, feature attribution, benchmark	As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on real-world datasets. In this work, we address this issue by releasing XAI-Bench: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and identifying surprising failure modes even for the most widely used explainers. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.
s6M0gjo0rL0	Rethinking the Role of Hyperparameter Tuning in Optimizer Benchmarking	https://openreview.net/forum?id=s6M0gjo0rL0	optimizer benchmarking	Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter configuration) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which may over-emphasize the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy.
dA2Iov0Ecxt	Cluster3D: A Dataset and Benchmark for Clustering Non-Categorical 3D CAD Models	https://openreview.net/forum?id=dA2Iov0Ecxt	3D CAD clustering, deep clustering, non-categorical, CAD models, ABC	We introduce the first large-scale dataset and benchmark for non-categorical annotation and clustering of 3D CAD models. We use the geometric data of the ABC dataset, and we develop an interface to allow expert mechanical engineers to efficiently annotate pairwise CAD model similarities, which we use to evaluate the performance of seven baseline deep clustering methods. Our dataset contains a manually annotated subset of 22,968 shapes, and 252,648 annotations. Our dataset is the first to directly target deep clustering algorithms for geometric shapes, and we believe it will be an important building block to analyze and utilize the massive 3D shape collections that are starting to appear in deep geometric computing. Our results suggest that, differently from the already mature shape classification algorithms, deep clustering algorithms for 3D CAD models are in their infancy and there is much room for improving their performance.
GVe2IvtZtVY	CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions	https://openreview.net/forum?id=GVe2IvtZtVY	intuitive physics, visual question answering, force dynamics, causal reasoning, temporal reasoning	Humans are able to perceive, understand and reason about physical events. Developing models with similar physical understanding capabilities is a long standing goal of artificial intelligence. As a step towards this goal, in this work, we introduce CRAFT, a new visual question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories from CRAFT include previously studied descriptive and counterfactual questions. Besides, inspired by the theories of force dynamics in cognitive linguistics, we introduce new question categories that involve understanding the interactions of objects through the notions of cause, enable, and prevent. Our results demonstrate that even though these tasks seem to be simple and intuitive for humans, the evaluated baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark dataset.
r8IvOsnHchr	Revisiting Time Series Outlier Detection: Definitions and Benchmarks	https://openreview.net/forum?id=r8IvOsnHchr	outlier detection, time series data, benchmark	Time series outlier detection has been extensively studied with many advanced algorithms proposed in the past decade. Despite these efforts, very few studies have investigated how we should benchmark the existing algorithms. In particular, using synthetic datasets for evaluation has become a common practice in the literature, and thus it is crucial to have a general synthetic criterion to benchmark algorithms. This is a non-trivial task because the existing synthetic methods are very different in different applications and the outlier definitions are often ambiguous. To bridge this gap, we propose a behavior-driven taxonomy for time series outliers and categorize outliers into point- and pattern-wise outliers with clear context definitions. Following the new taxonomy, we then present a general synthetic criterion and generate 35 synthetic datasets accordingly. We further identify 4 multivariate real-world datasets from different domains and benchmark 9 algorithms on the synthetic and the real-world datasets. Surprisingly, we observe that some classical algorithms could outperform many recent deep learning approaches. The datasets, pre-processing and synthetic scripts, and the algorithm implementations are made publicly available at https://github.com/datamllab/tods/tree/benchmark
FkDZLpK1Ml2	ATOM3D: Tasks on Molecules in Three Dimensions	https://openreview.net/forum?id=FkDZLpK1Ml2	machine learning, structural biology, biomolecules	Computational methods that operate on three-dimensional molecular structure have the potential to solve important questions in biology and chemistry. In particular, deep neural networks have gained significant attention, but their widespread adoption in the biomolecular domain has been limited by a lack of either systematic performance benchmarks or a unified toolkit for interacting with molecular data. To address this, we present ATOM3D, a collection of both novel and existing benchmark datasets spanning several key classes of biomolecules. We implement several classes of three-dimensional molecular learning methods for each of these tasks and show that they consistently improve performance relative to methods based on one- and two-dimensional representations. The specific choice of architecture proves to be critical for performance, with three-dimensional convolutional networks excelling at tasks involving complex geometries, graph networks performing well on systems requiring detailed positional information, and the more recently developed equivariant networks showing significant promise. Our results indicate that many molecular problems stand to gain from three-dimensional molecular learning, and that there is potential for improvement on many tasks which remain underexplored. To lower the barrier to entry and facilitate further developments in the field, we also provide a comprehensive suite of tools for dataset processing, model training, and evaluation in our open-source atom3d Python package. All datasets are available for download from www.atom3d.ai.
NevK78-K4bZ	The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions	https://openreview.net/forum?id=NevK78-K4bZ	behavior modeling, trajectory data, animal behavior	Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings.
Yx9jT3fkBaD	A Spoken Language Dataset of Descriptions for Speech-Based Grounded Language Learning	https://openreview.net/forum?id=Yx9jT3fkBaD	Grounded Language Acquisition, Speech Processing, Computer Vision, Natural Language Processing	Grounded language acquisition is a major area of research combining aspects of natural language processing, computer vision, and signal processing, compounded by domain issues requiring sample efficiency and other deployment constraints.        In this work, we present a multimodal dataset of RGB+depth objects with spoken as well as textual descriptions. We analyze the differences between the two types of descriptive language and our experiments demonstrate that the different modalities affect learning. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, depth, text, speech, and transcription interact, as well as how differences in the vernacular of these modalities impact results.
xfeO8f9gLT0	Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning	https://openreview.net/forum?id=xfeO8f9gLT0		Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves  are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this  objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order  to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.
mHrF3-r8-8t	Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions	https://openreview.net/forum?id=mHrF3-r8-8t	Bongard problems, human object interactions, HOI, visual reasoning, visual relationship reasoning, visual relational reasoning, few-shot learning, meta-learning	Visual relationship reasoning is central to how humans interpret the visual world and there are many dedicated benchmarks for this task. Although significant advances have been made on these benchmarks, there exists still a gap between human-level cognitive reasoning and machine-level pattern recognition that is limited by the training data and hard to generalize to unseen concepts. Inspired by the original one hundred Bongard problems (BPs), which has been a seminal benchmark for visual cognition tasks, we consider human-level concept learning on natural images with respect to human-object interactions. In this paper, we propose BPs for human-object interactions (Bongard-HOI), a new benchmark that emphasizes the cognitive reasoning of human-centric visual relationship. By constructing BPs with human-object relationships, Bongard-HOI inherits two desirable features from the original BPs and Bongard-LOGO: 1) few-shot concept learning and 2) context-dependent reasoning. Beyond these features, our benchmark studies systematic generalization in visual relationship reasoning by controlling the overlap levels of the object/action labels of human-object relationships between the training and testing sets. Our experiments show that the current machine learning models struggle on this benchmark while even amateur human testers on MTurk perform well. With Bongard-HOI, we hope to inspire further research efforts, especially on the systematic generalization, for real-world visual relationship reasoning.
VjFn0L5T7NU	RealCity3D: A Large-scale Georeferenced 3D Shape Dataset of Real-world Cities	https://openreview.net/forum?id=VjFn0L5T7NU	3D city, content generation, city generation, real world	Existing 3D shape datasets in the research community are generally limited to objects or scenes at the home level. City-level shape datasets are rare due to the difficulty in data collection and processing. However, such datasets uniquely present a new type of 3D data with a high variance in geometric complexity and spatial layout styles, such as residential/historical/commercial buildings and skyscrapers. This work focuses on collecting such data, and proposes city generation as new tasks for data-driven content generation. Thus, we collect over 1,000,000 geo-referenced 3D building models from New York City and Zurich. We benchmark various baseline performances on two challenging tasks: (1) city layout generation, and (2) building shape generation. Moreover, we propose an auto-encoding tree neural network for 2D building footprint and 3D building cuboid generation. The dataset, tools, and algorithms will be released to the community.
tjZjv_qh_CE	ARKitScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile RGB-D Data	https://openreview.net/forum?id=tjZjv_qh_CE	RGB-D Dataset, Indoor Scene Understanding, 3D Machine Learning, 3D Object Detection, Depth Upsampling, LiDAR	Scene understanding is an active research area. Commercial depth sensors such as Kinect, have enabled the release of several RGB-D datasets over the past few years which spawned novel methods in scene understanding. More recently with the launch of LiDAR sensor in Apple's iPads and iPhones, high quality RGB-D data is accessible to millions of people on a device they commonly use. This opens a whole new era in scene understanding for the Computer Vision community as well as the developers. The fundamental research in scene understanding together with the advances in machine learning can now impact people's everyday experiences. However transforming these fundamental scene understanding methods to everyday real world experiences, requires additional innovation and development.         In this paper we introduce ARKitScenes. ARKitScenes is not only the first RGB-D dataset that is captured with now widely available depth sensor, but also is the largest indoor scene understanding data ever collected. In addition to the raw and processed data, ARKitScenes includes high resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding boxes for a large taxonomy of furniture. We further analyze the usefulness of the data for two downstream tasks: 3D object detection and RGB-D guided upsampling. We demonstrate that our dataset can help push the boundaries of existing state-of-the-art methods and introduce new challenges that better represent real world scenarios.
dTxWk9K5KC	EEG Thinking1 Datasets: Think-Count-Recall (TCR) and Read-Write-Type (RWT)	https://openreview.net/forum?id=dTxWk9K5KC	dataset, EEG, machine learning, pipeline, user training, feedback	EEG-based Brain-Computer Interfaces (BCI) have been widely used in clinical and non-clinical research. In this paper, we present a pipeline to collect EEG data with non-invasive, wireless, and affordable hardware. We demonstrate an example of the datasets, Read-Write-Type (RWT). The framework of experimental design, data collection, data analysis, feedback generation, and community building could pave the way towards a future when everyone can easily use BCI systems every day, similar to smartphones nowadays.
1Y9fPheTgpp	Robustness Disparities in Commercial Face Detection	https://openreview.net/forum?id=1Y9fPheTgpp		  Facial detection and analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Critiques that focus on system performance analyze disparity of the system's output, i.e., how frequently is a face detected for different Fitzpatrick skin types or perceived genders. However, we focus on the robustness of these system outputs under noisy natural perturbations. We present the first of its kind detailed benchmark of the robustness of two such systems: Amazon Rekognition and Microsoft Azure. Using both standard and recently released academic facial datasets, we find that photos of individuals who are \emph{older}, \emph{masculine presenting}, of \emph{darker skin type}, or have \emph{dim lighting} are more susceptible to errors than their counterparts in other identities.
oUg5rC_95OM	A Benchmark of Medical Out of Distribution Detection	https://openreview.net/forum?id=oUg5rC_95OM		Motivation: Deep learning models deployed on medical tasks can be equipped with Out-of-Distribution Detection (OoDD) methods in order to avoid erroneous predictions. However it is unclear which OoDD methods are effective in practice.               Specific Problem: Systems trained for one particular domain of images cannot be expected to perform accurately on images of a different domain.  These images should be flagged by an OoDD method prior to prediction.               Our approach: This paper defines 3 categories of OoD examples and benchmarks popular OoDD methods in three domains of medical imaging: chest X-ray, fundus imaging, and histology slides.               Results: Our experiments show that despite methods yielding good results on some categories of out-of-distribution samples, they fail to recognize images close to the training distribution.              Conclusion: We find a simple binary classifier on the feature representation has the best accuracy and AUPRC on average. Users of diagnostic tools which employ these OoDD methods should still remain vigilant that images very close to the training distribution yet not in it could yield unexpected results.
pY9MHwmrymR	An Extensible Benchmark Suite for Learning to Simulate Physical Systems	https://openreview.net/forum?id=pY9MHwmrymR	Scientific Computing, Physics and ML, Numerical Integration, Physical Simulation	 Simulating physical systems is a core component of scientific computing, encompassing a wide range of physical domains and applications. Recently, there has been a surge in data-driven methods to complement traditional numerical simulations methods, motivated by the opportunity to reduce computational costs and/or learn new physical models leveraging access to large collections of data. However, the diversity of problem settings and applications has led to a plethora of approaches, each one evaluated on a different setup and with different evaluation metrics.        We introduce a set of benchmark problems to take a step towards unified benchmarks and evaluation protocols. We propose four representative physical systems, as well as a collection of both widely used classical time integrators and representative data-driven methods (kernel-based, MLP, CNN, Nearest-Neighbors). Our framework allows to evaluate objectively and systematically the stability, accuracy, and computational efficiency of data-driven methods. Additionally, it is configurable to permit adjustments for accommodating other learning tasks and for establishing a foundation for future developments in machine learning for scientific computing.
pMWtc5NKd7V	RadGraph: Extracting Clinical Entities and Relations from Radiology Reports	https://openreview.net/forum?id=pMWtc5NKd7V	natural language processing, radiology, entity and relation extraction, multi-modal, graph	Extracting structured clinical information from free-text radiology reports can enable the use of radiology report information for a variety of critical healthcare applications. In our work, we present RadGraph, a dataset of entities and relations in full-text chest X-ray radiology reports based on a novel information extraction schema we designed to structure radiology reports. We release a development dataset, which contains board-certified radiologist annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and a test dataset, which contains two independent sets of board-certified radiologist annotations for 100 radiology reports split equally across the MIMIC-CXR and CheXpert datasets. Using these datasets, we train and test a deep learning model, RadGraph Benchmark, that achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR and CheXpert test sets respectively. Additionally, we release an inference dataset, which contains annotations automatically generated by RadGraph Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4 million relations) and 500 CheXpert reports (13,783 entities and 9,908 relations) with mappings to associated chest radiographs. Our freely available dataset can facilitate a wide range of research in medical natural language processing, as well as computer vision and multi-modal learning when linked to chest radiographs.
LZ5cx2yismf	FedScale: Benchmarking Model and System Performance of Federated Learning	https://openreview.net/forum?id=LZ5cx2yismf	federated learning, benchmark, evaluation platform	We present FedScale, a diverse set of challenging and realistic benchmark datasets to facilitate scalable, comprehensive, and reproducible federated learning (FL) research. FedScale datasets are large-scale, encompassing a diverse range of important FL tasks, such as image classification, object detection, language modeling, speech recognition, and reinforcement learning. For each dataset, we provide a unified evaluation protocol using realistic data splits and evaluation metrics. To meet the pressing need for reproducing realistic FL at scale, we have also built an efficient evaluation platform to simplify and standardize the process of FL experimental setup and model evaluation. Our evaluation platform provides flexible APIs to implement new FL algorithms and includes new execution backends with minimal developer efforts. Finally, we perform indepth benchmark experiments on these datasets. Our experiments suggest fruitful opportunities in heterogeneity-aware co-optimizations of the system and statistical efficiency under realistic FL characteristics. FedScale is open-source with permissive licenses and actively maintained, and we welcome feedback and contributions from the community.
R8CwidgJ0yT	The People’s Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage	https://openreview.net/forum?id=R8CwidgJ0yT	speech recognition, dataset, forced alignment, creative commons, supervised learning	The People’s Speech is a free-to-download 31,400-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under CC-BY-SA. The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions. We describe our data collection methodology and release our data collection system under the Apache2.0 license. We show that a model trained on this dataset achieves a 32.17% word error rate on Librispeech’s test-clean test set.  Finally, we discuss the legal and ethical issues surrounding the creation of a sizable machine learning corpora and plans for continued maintenance of the project under MLCommons’s sponsorship.
8nvgnORnoWr	Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development	https://openreview.net/forum?id=8nvgnORnoWr	drug discovery and development, representation learning, distribution shift, low-resource learning, datasets and benchmarks	Therapeutics machine learning is an emerging field with incredible opportunities for innovation and impact. However, advancement in this field requires formulation of meaningful learning tasks and careful curation of datasets. Here, we introduce Therapeutics Data Commons (TDC), the first unifying platform to systematically access and evaluate machine learning across the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets spread across 22 learning tasks and spanning the discovery and development of safe and effective medicines. TDC also provides an ecosystem of tools and community resources, including 33 data functions and types of meaningful data splits, 23 strategies for systematic model evaluation, 17 molecule generation oracles, and 29 public leaderboards. All resources are integrated and accessible via an open Python library. We carry out extensive experiments on selected datasets, demonstrating that even the strongest algorithms fall short of solving key therapeutics challenges, including real dataset distributional shifts, multi-scale modeling of heterogeneous data, and robust generalization to novel data points. We envision that TDC can facilitate algorithmic and scientific advances and considerably accelerate machine-learning model development, validation and transition into biomedical and clinical implementation. TDC is an open-science initiative available at https://tdcommons.ai.
yl9aThYT9W	All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds	https://openreview.net/forum?id=yl9aThYT9W	Autonomous Driving, Perception, Datasets	Developing datasets that cover comprehensive sensors, annotations, and out-of-distribution data is important for innovating robust multi-sensor multi-task perception systems in autonomous driving. Though many datasets have been released, they target different use-cases such as 3D segmentation (SemanticKITTI), radar data (nuScenes), large-scale training and evaluation (Waymo). As a result, we are still in need of a dataset that forms a union of various strengths of existing datasets. To address this challenge, we present the AIODrive dataset, a synthetic large-scale dataset that provides comprehensive sensors, annotations, and environmental variations. Specifically, we provide (1) eight sensor modalities (RGB, Stereo, Depth, LiDAR, SPAD-LiDAR, Radar, IMU, GPS), (2) annotations for all mainstream perception tasks (e.g., detection, tracking, prediction, segmentation, depth estimation, etc), and (3) out-of-distribution driving scenarios such as adverse weather and lighting, crowded scenes, high-speed driving, violation of traffic rules, and vehicle crash. In addition to comprehensive data, long-range perception is also important to perception systems as early detection of faraway objects can help prevent collision in high-speed driving scenarios. However, due to the sparsity and limited range of point cloud data in prior datasets, developing and evaluating long-range perception algorithms is not feasible. To address the issue, we provide high-density long-range point clouds for LiDAR and SPAD-LiDAR sensors (10x than Velodyne-64), to enable research in long-range perception. Our dataset is released and free to use for both research and commercial purpose: http://www.aiodrive.org/
8RxxwAut1BI	MLPerf Tiny Benchmark	https://openreview.net/forum?id=8RxxwAut1BI	Machine learning, Benchmark, Embedded, IoT, Neural Network, Ultra-Low-Power	Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the first industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reflects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the benefits of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classification, and anomaly detection.
Blmvlo-yRR9	High-Dimensional Gene Expression and Morphology Profiles of Cells across 28,000 Genetic and Chemical Perturbations	https://openreview.net/forum?id=Blmvlo-yRR9		Populations of cells can be perturbed by various chemical and genetic treatments and the impact on the cells’ gene expression (transcription, i.e. mRNA levels) and morphology (in an image-based assay) can be measured in high dimensions. The patterns observed in this data can be used for more than a dozen applications in drug discovery and basic biology research. We provide a collection of four datasets where both gene expression and morphological data are available; roughly a thousand features are measured for each data type, across more than 28,000 thousand chemical and genetic perturbations. We have defined a set of biological problems that can be investigated using these two data modalities and provided baseline analysis and evaluation metrics for addressing each. This data resource is available at http://broad.io/rosetta.
1xDTDk3XPW	A Large-Scale Database for Graph Representation Learning	https://openreview.net/forum?id=1xDTDk3XPW	graph representation learning, graph classification, dataset, database, graphs	With the rapid emergence of graph representation learning, the construction of new large-scale datasets are necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all of these desired properties. We introduce MalNet , the largest public graph database ever constructed, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 44x larger graphs on average, and 63x more classes. We provide a detailed analysis of MalNet, discussing its properties and provenance, along with the evaluation of state-of-the-art machine learning and graph neural network techniques. The unprecedented scale and diversity of MalNet offers exciting opportunities to  advance the frontiers of graph representation learning--enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publicly available at www.mal-net.org.
eOOiCyZ_h9f	StudentSADD: Mobile Depression and Suicidal Ideation Screening of College Students during the Coronavirus Pandemic	https://openreview.net/forum?id=eOOiCyZ_h9f	mobile health, mental health assessment, transfer learning, digital phenotype	The growing prevalence of depression and suicidal ideation among college students is alarming, with the Coronavirus pandemic  further highlighting the need for universal mental illness screening technology. While traditional screening questionnaires are too burdensome to achieve universal screening in this population, data collected through mobile applications has the potential to identify at-risk students. However, knowing the modalities that students are  willing to share and that contain strong screening capabilities is critical for developing such mental illness screening technology. Thus, we deployed a mobile application to over 300 students during the pandemic to collect the Student Suicidal Ideation and Depression Detection (StudentSADD) dataset. Overall, students were most willing to share text responses, unscripted voice recordings, and scripted voice recordings. To provide baselines, we trained machine learning and deep learning methods on these modalities to screen for depression and suicidal ideation. The novel  StudentSADD dataset is a valuable resource for developing mobile mental illness screening technologies.
EHfBhk6IkYF	Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations	https://openreview.net/forum?id=EHfBhk6IkYF	hierarchy, composition, dataset, hangul, images	Hierarchy and compositionality are common latent properties in many natural and scientific datasets. Determining when a deep network's hidden activations represent hierarchy and compositionality is important both for understanding deep representation learning and for applying deep networks in domains where interpretability is crucial. However, current benchmark machine learning datasets either have little hierarchical or compositional structure, or the structure is not known. This gap impedes precise analysis of a network's representations and thus hinders development of new methods that can learn such properties. To address this gap, we developed a new benchmark dataset with known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD) is comprised of 35 fonts from the Korean writing system (Hangul), each with 11,172 blocks (syllables) composed from the product of initial consonant, medial vowel, and final consonant glyphs. All blocks can be grouped into a few geometric types which induces a hierarchy across blocks. In addition, each block is composed of individual glyphs with rotations, translations, scalings, and naturalistic style variation across fonts. We find that both shallow and deep unsupervised methods only show modest evidence of hierarchy and compositionality in their representations of the HFD compared to supervised deep networks. Supervised deep network representations contain structure related to the geometrical hierarchy of the characters, but the compositional structure of the data is not evident. Thus, HFD enables the identification of shortcomings in existing methods, a critical first step toward developing new machine learning algorithms to extract hierarchical and compositional structure in the context of naturalistic variability.
LqRSh6V0vR	Reinforcement Learning Benchmarks for Traffic Signal Control	https://openreview.net/forum?id=LqRSh6V0vR		We propose a toolkit for developing and comparing reinforcement learning (RL)-based traffic signal controllers. The toolkit includes implementation of state-of-the-art deep-RL algorithms for signal control along with benchmark control problems that are based on realistic traffic scenarios. Importantly, the toolkit allows a first-of-its-kind comparison between state-of-the-art RL-based signal controllers while providing benchmarks for future comparisons. Consequently, we compare and report the relative performance of current RL algorithms. The experimental results suggest that previous algorithms are not robust to varying sensing assumptions and non-stylized intersection layouts. When more realistic signal layouts and advanced sensing capabilities are assumed, a distributed deep-Q learning approach is shown to outperform previously reported state-of-the-art algorithms in many cases.
db1InWAwW2T	ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation	https://openreview.net/forum?id=db1InWAwW2T	Interactive Physical Simulation, Virtual Environment, Multi-modal	We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables the simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable ``avatars” that embody AI agents; and support for human interactions with VR devices. TDW’s API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that ‘learn like a child’, and attention studies in humans and neural networks.
CXyZrKPz4CU	Physion: Evaluating Physical Prediction from Vision in Humans and Machines	https://openreview.net/forum?id=CXyZrKPz4CU		While machine learning algorithms excel at many challenging visual tasks, it is unclear that they can make predictions about commonplace real world physical events. Here, we present a visual and physical prediction benchmark that precisely measures this capability. In realistically simulating a wide variety of physical phenomena – rigid and soft-body collisions, stable multi-object configurations, rolling and sliding, projectile motion – our dataset presents a more comprehensive challenge than existing benchmarks. Moreover, we have collected human responses for our stimuli so that model predictions can be directly compared to human judgements. We compare an array of algorithms – varying in their architecture, learning objective, input-output structure, and training data – on their ability to make diverse physical predictions. We find that graph neural networks with access to the physical state best capture human behavior, whereas among models that receive only visual input, those with object-centric representations or pretraining do best but fall far short of human accuracy. This suggest that extracting physically meaningful representations of scenes is the main bottleneck to achieving human-like visual prediction. We thus demonstrate how our benchmark can identify areas for improvement and measure progress on this key aspect of physical understanding.
vDilkBNNbx6	CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms	https://openreview.net/forum?id=vDilkBNNbx6		Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favourable outcomes in the future (e.g., insurance approval).       Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available open--source implementations, the decision in favour of certain models is primarily based on what is readily available. Going forward -- to guarantee meaningful comparisons across explanation methods -- we present \texttt{CARLA} (\textbf{C}ounterfactual \textbf{A}nd \textbf{R}ecourse \textbf{L}ibr\textbf{A}ry), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods.       We have open sourced \texttt{CARLA} and our experimental results on \href{https://github.com/indyfree/CARLA}{Github}, making them available as competitive baselines. We welcome contributions from other research groups and practitioners.
qeM58whnpXM	It's COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks	https://openreview.net/forum?id=qeM58whnpXM	algorithmic fairness, risk assessment, benchmark, criminal justice, COMPAS	Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS dataset, are commonly used in algorithmic fairness papers due to benchmarking practices of comparing algorithms on datasets used in prior work. In many cases, this data is used as a benchmark to demonstrate good performance without accounting for the complexities of criminal justice (CJ) processes. We show that pretrial RAI datasets contain numerous measurement biases and errors inherent to CJ pretrial evidence and due to disparities in discretion and deployment, are limited in making claims about real-world outcomes, making the datasets a poor fit for benchmarking under assumptions of ground truth and real-world impact. Conventional practices of simply replicating previous data experiments may implicitly inherit or edify normative positions without explicitly interrogating assumptions. With context of how interdisciplinary fields have engaged in CJ research, algorithmic fairness practices are misaligned for meaningful contribution in the context of CJ, and would benefit from transparent engagement with normative considerations and values related to fairness, justice, and equality. These factors prompt questions about whether benchmarks for intrinsically socio-technical systems like the CJ system can exist in a beneficial and ethical way.
erOBVUgvryF	JECC: Commonsense Reasoning Tasks Derived fromInteractive Fictions	https://openreview.net/forum?id=erOBVUgvryF	text games, commonsense reasoning, text entailment, natural language processing	Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction  (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Experiments show that the introduced dataset is challenging to previous machine reading models with a significant 20% performance gap compared to human experts.
JNvy4qBcxHW	VoxelScape: Large Scale Simulated 3D Point Cloud Dataset of Urban Traffic Environments	https://openreview.net/forum?id=JNvy4qBcxHW	Realistically simulated point cloud dataset	Having a profound understanding of the surrounding environment is considered one of the crucial tasks for the reliable operation of future self-driving cars. Light Detection and Ranging (LiDAR) sensor plays a critical role in achieving such understanding due to its capability to perceive the world in 3D. Similar to 2D perception tasks, current state-of-the-art methods in 3D perception tasks rely on deep neural networks (DNNs). However, the performance of 3D perception tasks, specially point-wise semantic segmentation, is not on par with their 2D counterparts. One of the main reasons is the lack of publicly available labelled 3D point cloud datasets (PCDs) from 3D LiDAR sensors. In this work, we are introducing the VoxelScape dataset, a large-scale simulated 3D PCD with 100K annotated point cloud scans. The annotations in the VoxelScape dataset includes both point-wise semantic labels and 3D bounding boxes labels. Additionally, we used a number of baseline approaches to validate the transferability of VoxelScape to real 3D PCD on the 3D semantic segmentation task. The promising results have shown that training DNNs on VoxelScape boosted the performance of the 3D semantic segmentation task on the real PCD. The VoxelScape dataset is publicly available through https://voxel-scape.github.io/dataset/
CSi1eu_2q96	Automatic Construction of Evaluation Suites for Natural Language Generation Datasets	https://openreview.net/forum?id=CSi1eu_2q96	natural language processing, natural language generation, benchmark construction, evaluation	Machine learning approaches applied to NLP are often evaluated by summarizing their performance in a single number, for example accuracy. Since most test sets are constructed as an i.i.d. sample from the overall data, this approach overly simplifies the complexity of language and encourages overfitting to the head of the data distribution. As such, rare language phenomena or text about underrepresented groups are not equally included in the evaluation. To encourage more in-depth model analyses, researchers have proposed the use of multiple test sets, also called challenge sets, that assess specific capabilities of a model. In this paper, we develop a framework based on this idea which is able to generate controlled perturbations and identify subsets in text-to-scalar, text-to-text, or data-to-text settings. By applying this framework to the GEM generation benchmark, we propose an evaluation suite made of 80 challenge sets, demonstrate the kinds of analyses that it enables and shed light onto the limits of current generation models.
m28E5RN64hi	RealCause: Realistic Causal Inference Benchmarking	https://openreview.net/forum?id=m28E5RN64hi	Causa inference, benchmarking	There are many different causal effect estimators in causal inference. However, it is unclear how to choose between these estimators because there is no ground-truth for causal effects. A commonly used option is to simulate synthetic data, where the ground-truth is known. However, the best causal estimators on synthetic data are unlikely to be the best causal estimators on real data. An ideal benchmark for causal estimators would both (a) yield ground-truth values of the causal effects and (b) be representative of real data. Using flexible generative models, we provide a benchmark that both yields ground-truth and is realistic. Using this benchmark, we evaluate over 1500 different causal estimators and provide evidence that it is rational to choose hyperparameters for causal estimators using predictive metrics.
eYg8ssXm3BT	A Framework for Cluster and Classifier Evaluation in the Absence of Reference Labels	https://openreview.net/forum?id=eYg8ssXm3BT	ground truth refinement, reference labels	In some problem spaces the high cost of obtaining ground truth labels necessitates use of lower quality reference datasets.        It is difficult to benchmark model changes using these datasets, as evaluation results may be misleading or biased. We propose a supplement to using reference labels which we call an approximate ground truth refinement (AGTR). Using an AGTR we prove that bounds on the precision and recall of a clustering algorithm or multiclass classifier can be computed without reference labels. We introduce a litmus test that uses an AGTR to identify inaccurate evaluation results produced from reference datasets of dubious quality. Creating an AGTR requires domain knowledge, and malware family classification is a task with robust domain knowledge approaches that support the construction of an AGTR. We demonstrate our AGTR evaluation framework by applying it to a popular malware labeling tool to diagnose over-fitting in prior testing and evaluate changes that could not be meaningfully quantified in their impact under previous data.
Q0hm0_G1mpH	A Unified Few-Shot Classification Benchmark to Compare Transfer and Meta Learning Approaches	https://openreview.net/forum?id=Q0hm0_G1mpH	transfer learning, meta-learning, few-shot classification	Meta and transfer learning are two successful families of approaches to few-shot learning. Despite highly related goals, state-of-the-art advances in each family are measured largely in isolation of each other. As a result of diverging evaluation norms, a direct or thorough comparison of different approaches is challenging. To bridge this gap, we introduce a few-shot classification evaluation protocol named VTAB+MD with the explicit goal of facilitating sharing of insights from each community. We demonstrate its accessibility in practice by performing a cross-family study of the best transfer and meta learners which report on both a large-scale meta-learning benchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual Task Adaptation Benchmark, VTAB). We find that, on average, large-scale transfer methods (Big Transfer, BiT) outperform competing approaches on MD, even when trained only on ImageNet. In contrast, meta-learning approaches struggle to compete on VTAB when trained and validated on MD. However, BiT is not without limitations, and pushing for scale does not improve performance on highly out-of-distribution MD tasks. We hope that this work contributes to accelerating progress on few-shot learning research.
Qd_eU1wvJeu	Addressing "Documentation Debt" in Machine Learning: A Retrospective Datasheet for BookCorpus	https://openreview.net/forum?id=Qd_eU1wvJeu	bookcorpus, datasheet, dataset, documentation, data, text	Recent work has underscored the importance of dataset documentation in machine learning research, including by addressing "documentation debt" for datasets that have been used widely but documented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text dataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, even though little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a retrospective datasheet that provides key context and information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence that (1) BookCorpus violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, such as lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus offers a cautionary tale and adds to growing literature that urges more careful, systematic documentation of machine learning datasets.
fl5PtMeGJ8z	CatLC: Catalonia Multiresolution Land Cover Dataset	https://openreview.net/forum?id=fl5PtMeGJ8z	remote sensing, land cover, dataset, segmentation	We live in the era of datafication and, as such, datasets are created with many different purposes. Traditional natural image datasets are very rich. However, only a few remote sensing datasets are available, and the few that exist either cover a tiny territory or cover a larger one with low spacial resolution and/or few classes. Hence we present the Catalonia Multiresolution Land Cover Dataset (CatLC), a remote sensing dataset that contains images at different spatial resolutions captured by both aircraft and satellites (Sentinel-1 and Sentinel-2), in addition to topographic maps. All this dataset has been created with images from the catalogs of the Cartographic and Geological Institute of Catalonia (ICGC) and the European Space Agency (ESA). The ICGC's land cover ground truth accompanies these images with 41 classes at a spatial resolution of 1 m in an area of 32000 km2, covering the Spanish region of Catalonia. CatLC is a multilayer, multiresolution, multimodal, multitemporal dataset, and as such, has excellent potential for the Artificial Intelligence (AI) community and the exploration of modeling methodologies. Land cover maps are used in different realms such as forestry for inventory area estimates, hydrology regarding microclimatic variables, agriculture to improve irrigation or geology in geohazards and risk identification and assessment. Therefore, accurate and updated knowledge about land changes are essential for territory management with different purposes over multiple fields. Land cover classification and change detection are among the primary uses of airborne and satellite images and constitute the focus of this work. In this paper, we also include an initial benchmark as a guide for the correct use of this dataset. In all, CatLC provides a complete dataset acquired by airborne and satellite sensors and topographic maps at different frequency bands and spatial resolutions, making it an extraordinary dataset for multiple AI and deep learning applications based on remote sensing data.
J0d-I8yFtP	The Neural MMO Platform for Massively Multiagent Research	https://openreview.net/forum?id=J0d-I8yFtP	Environment, Multiagent, Reinforcement Learning, Simulation, MMO, Platform	Neural MMO is a computationally accessible research platform that combines large agent populations, long time horizons, open-ended tasks, and modular game systems. Existing environments feature subsets of these properties, but Neural MMO is the first to combine them all. We present Neural MMO as free and open source software with active support, ongoing development, documentation, and additional training, logging, and visualization tools to help users adapt to this new setting. Initial baselines on the platform demonstrate that agents trained in large populations explore more and learn a progression of skills. We raise other more difficult problems such as many-team cooperation as open research questions which Neural MMO is well-suited to answer. Finally, we discuss current limitations of the platform, potential mitigations, and plans for continued development.
nvPp2SomA98	MMVText: A Large-Scale, Multidimensional Multilingual Dataset for Video Text Spotting	https://openreview.net/forum?id=nvPp2SomA98	video text spotting, text detection and recognition, video understanding with text	Video text spotting is crucial for numerous real application scenarios, but most existing video text reading benchmarks are challenging to evaluate the performance of advanced deep learning algorithms due to the limited amount of training data and tedious scenarios. To address this issue, we introduce a new large-scale benchmark dataset named Multidimensional Multilingual Video Text (MMVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. There are mainly three features for MMVText. Firstly, we provide 510 videos with more than 1,000,000 frame images, four times larger than the existing largest dataset for text in videos. Secondly, our dataset covers 30 open categories with a wide selection of various scenarios, life vlog, sports news, automatic drive, cartoon, etc. Besides, caption text and scene text are separately tagged for the two different representational meanings in the video. The former represents more theme information, and the latter is the scene information. Thirdly, the MMVText provides multilingual text annotation to promote multiple cultures live and communication. In the end, a comprehensive experimental result and analysis concerning text detection, recognition, tracking, and end-to-end spotting on MMVText are provided. We also discuss the potentials of using MMVText for other video-and-text research.
Wb4vR9NPE6_	Turath-150K: Image Database of Arab Heritage	https://openreview.net/forum?id=Wb4vR9NPE6_	Image database, heritage, cultural diversity	Large-scale image databases remain largely biased towards objects and activities encountered in a select few cultures. This absence of culturally-diverse images, which we refer to as the \enquote{hidden tail}, limits the applicability of pre-trained neural networks and inadvertently excludes researchers from under-represented regions. To begin remedying this issue, we curate Turath-150K, a database of images of the Arab world that reflect objects, activities, and scenarios commonly found there. In the process, we introduce three benchmark databases, Turath Standard, Art, and UNESCO, specialised subsets of the Turath dataset. After demonstrating the limitations of existing networks pre-trained on ImageNet when deployed on such benchmarks, we train and evaluate several networks on the task of image classification. As a consequence of Turath, we hope to engage machine learning researchers in under-represented regions, and to inspire the release of additional culture-focused databases. The database can be accessed here: \url{danikiyasseh.github.io/Turath}.
qBl8hnwR0px	Which priors matter? Benchmarking models for learning latent dynamics	https://openreview.net/forum?id=qBl8hnwR0px		Learning dynamics is at the heart of many important applications of machine learning (ML), such as robotics and autonomous driving.  In these settings, ML algorithms typically need to reason about a physical system using high dimensional observations, such as images, without access to the underlying state. Recently, several methods have proposed to integrate priors from classical mechanics into ML models to address the challenge of physical reasoning from images.  In this work, we take a sober look at the current capabilities of these models. To this end, we introduce a suite consisting of 17 datasets with visual observations        based on physical systems exhibiting a wide range of dynamics.  We conduct a thorough and detailed comparison of the major classes of physically inspired methods alongside several strong baselines. While models that incorporate physical priors can often learn latent spaces with desirable properties, our results demonstrate that these methods fail to significantly improve upon standard techniques. Nonetheless, we find that the use of continuous and time-reversible dynamics benefits models of all classes.
ls__H1FcLwv	Age dataset: age of death and other basic information for over 1.31 million historical figures (data, methods and applications)	https://openreview.net/forum?id=ls__H1FcLwv	historical figures, demography, famous people, gender prediction, occupation, dataset	In this paper, we present a novel dataset on the life, work, and death of 1,318,331 notable deceased people. The dataset provides full name, short description, birth year, gender, country, occupation, death year, death manner, and age of death in tabular format. The base data is collected from RDF triples hosted by the Wikimedia Foundation and has been carefully cleaned and edited to provide more consistent and accurate values. We utilized a text mining method on the short descriptions to extract birth and death year; and calculated the age of death for all individuals (100%). We used additional analytical and text mining methods to reduce the number of missing values for the binary gender and occupation. The dataset contains people from various social groups, including but not limited to 1M males, 118k females, 135 non-binary people, 303k artists, 207k politicians, 118k athletes, and 99k researchers. We present potential applications of the dataset in several areas of study, including historical, cultural and cross-cultural studies, health-related research, death prediction, and popularity ranking tasks, and world event analysis.
_-O9SefMb99	LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptation Semantic Segmentation	https://openreview.net/forum?id=_-O9SefMb99	Land-cover mapping, Unsupervised domain adaptation, Semantic segmentation	Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets only focus on improvement of the semantic segmentation in one domain (urban or rural), thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptation semantic segmentation (LoveDA) dataset to promote large-scale land-cover mapping. The LoveDA dataset contains 3338 aerial images with 86,516 annotated objects for seven common land-cover categories. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the: 1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on nine semantic segmentation methods and eight UDA methods. Some exploratory studies were also carried out to find alternative ways to address these challenges.
JjuthZiKQ8H	SkillBERT: "Skilling" the BERT to classify skills using Electronic Recruitment Records	https://openreview.net/forum?id=JjuthZiKQ8H	Competency, Electronic Recruitment Record, Recruitment, SkillBERT, Skill embeddings, Skill semantics	In this work, we show how the Electronic Recruitment Records (ERRs) that store the information related to job postings and candidates can be mined and analyzed to provide assistance to hiring managers in recruitment. These ERRs are captured through our recruitment portal, where hiring managers can post the jobs and candidates can apply for various job postings. These ERRs are stored in the form of tables in our recruitment database and whenever there is a new job posting, a new ERR is added to the database.       We have leveraged the skills present in the ERRs to train a BERT-based model, SkillBERT, the embeddings of which are used as features for classifying skills into groups referred to as “competency groups”. A competency group is a group of similar skills, and it is used as matching criteria (instead of matching on skills) for finding the overlap of skills between the candidates and the jobs. This proxy match takes advantage of the BERT’s capability of deriving meaning from the structure of competency groups present in the skill dataset. The skill classification is a multi-label classification problem as a single skill can belong to multiple competency groups. To solve multi-label competency group classification using a binary classifier, we have paired each skill with each competency group and tried to predict the probability of that skill belonging to that particular competency group. SkillBERT, which is trained from scratch on the skills present in job requisitions, is shown to be better performing than the pre-trained BERT (Devlin et al., 2019) and the Word2Vec (Mikolov et al., 2013). We have also explored K-means clustering (Lloyd, 1982) and spectral clustering (Chung, 1997) on SkillBERT embeddings to generate cluster-based features. Both algorithms provide similar performance benefits. Last, we have experimented with different classification models like Random Forest (Breiman, 2001), XGBoost (Chen and Guestrin, 2016), and a deep learning algorithm Bi-LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997) for the tagging of competency groups to skill. We did not observe a significant performance difference among the algorithms, although XGBoost and Bi-LSTM perform slightly better than Random Forest. The features created using SkillBERT are most predictive in the classification task, which demonstrates that the SkillBERT is able to capture the information pertaining to skill ontology from the data. We have made the source code, the trained models, and the dataset.
Q0SbORK5zoN	A ground-truth dataset of real security patches	https://openreview.net/forum?id=Q0SbORK5zoN	Security Vulnerabilities, Commits, Natural Language Artifacts, Code Changes	Training machine learning approaches for vulnerability identification and producing reliable tools to assist developers in implementing quality software—free of vulnerabilities—is challenging due to the lack of large datasets and real data. Researchers have been looking at these issues and building datasets.  However, these datasets usually miss natural language artifacts and programming language diversity. We scrapped the entire CVE details database for GitHub references and augmented the data with 3 security-related datasets.  We used the data to create a ground-truth dataset of natural language artifacts (such as commit messages, commits comments, and summaries), meta-data and code changes. Our dataset integrates a total of 8057 security-relevant commits—the equivalent to 5942 security patches—from 1339 different projects spanning 146 different types of vulnerabilities and 20 languages.  A dataset of 110k non-security-related commits is also provided. Data and scripts are all available on GitHub. Data is stored in a .CSV file.  Codebases can be downloaded using our scripts.  Our dataset is a valuable asset to answer research questions on different topics such as the identification of security-relevant information using NLP models; software engineering and security best practices; and, vulnerability detection and patching; and, security program analysis.
ASeMiqai-k1	HypeML: A Health AI Promotional Language Dataset	https://openreview.net/forum?id=ASeMiqai-k1	AI, machine learning, hype, NLP	Recent advances in AI have the potential to significantly improve healthcare in a variety of clinical areas. However, many are concerned that biomedical publications about health AI may overstate the applicability and utility of evaluated systems. To ensure that the latest AI technologies are used effectively and ethically in clinical contexts, it is essential that healthcare providers are able to understand and detect hype in biomedical publications about health AI. To assist with these efforts, we present HypeML, a groundtruthed dataset of promotional claims in health AI and related AI performance data. HypeML provides benchmarking data for AI systems evaluated in 82 studies and promotional language annotations for 922 sentences from those study abstracts. The data are available at https://github.com/sscottgraham/HypeML.
Zkj_VcZ6ol	ImageNet-21K Pretraining for the Masses	https://openreview.net/forum?id=Zkj_VcZ6ol	ImageNet21K, pretraining, SOTA, semantic softmax, single-label, multi-label, downstream	ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value.       This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone.       Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models.        We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer.       Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at:  https://github.com/Alibaba-MIIL/ImageNet21K
Jvxa8adr3iY	NaturalProofs: Mathematical Theorem Proving in Natural Language	https://openreview.net/forum?id=Jvxa8adr3iY	NLP, mathematics, retrieval, generation, reasoning, theorem proving	Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NaturalProofs, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NaturalProofs unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization. Using NaturalProofs, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system's ability to determine key results that appear in a proof. Large-scale sequence models show promise compared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement. NaturalProofs opens many avenues for research on challenging mathematical tasks.
I01l7rc0jcb	Monash Time Series Forecasting Archive	https://openreview.net/forum?id=I01l7rc0jcb	global time series forecasting, benchmark datasets, feature analysis, baseline evaluation	Many businesses nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models and multivariate models that are trained across sets of time series have shown huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series forecasting archives that contain datasets of time series from similar sources available for researchers to evaluate the performance of new global or multivariate forecasting algorithms over varied datasets. In this paper, we present such a comprehensive forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across ten error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.
DAkP1TT_Ubm	ZeroWaste Dataset: Towards Automated Waste Recycling	https://openreview.net/forum?id=DAkP1TT_Ubm	dataset, recycling, segmentation, detection	Less than 35% of recyclable waste is being actually recycled in the US, which leads to increased soil and sea pollution and is one of the major concerns of environmental researchers as well as the common public. At the heart of the problem is the inefficiencies of the waste sorting process (separating paper, plastic, metal, glass, etc.) due to the extremely complex and cluttered nature of the waste stream. Automated waste detection strategies have a great potential to enable more efficient, reliable and safer waste sorting practices, but the literature lacks comprehensive datasets and methodology for the industrial waste sorting solutions. In this paper, we take a step towards computer-aided waste detection and present the first in-the-wild industrial-grade waste detection and segmentation dataset, ZeroWaste. This dataset contains over 1800 fully segmented video frames collected from a real waste sorting plant along with waste material labels for training and evaluation of the segmentation methods, as well as over 6000 unlabeled frames that can be further used for semi-supervised and self-supervised learning techniques. ZeroWaste also provides frames of the conveyor belt before and after the sorting process, comprising a novel setup that can be used for weakly-supervised segmentation. We present baselines for fully-, semi- and weakly-supervised segmentation methods. Our experimental results demonstrate that state-of-the-art segmentation methods struggle to correctly detect and classify target objects which suggests the challenging nature of our proposed in-the-wild dataset. We believe that ZeroWaste will catalyze research in object detection and semantic segmentation in extreme clutter as well as applications in the recycling domain. Our project page can be found at http://ai.bu.edu/zerowaste/.
IsYUDnbIqay	Modeling and Optimizing Laser-Induced Graphene	https://openreview.net/forum?id=IsYUDnbIqay	materials science, graphene, Bayesian optimization	A lot of technological advances depend on next-generation materials, such as graphene, which enables a raft of new applications, for example better electronics. Manufacturing such materials is often difficult; in particular, producing graphene at scale is an open problem. We provide a series of datasets that describe the optimization of the production of laser-induced graphene, an established manufacturing method that has shown great promise. We pose three challenges based on the datasets we provide -- modeling the behavior of laser-induced graphene production with respect to parameters of the production process, transferring models and knowledge between different precursor materials, and optimizing the outcome of the transformation over the space of possible production parameters. We present illustrative results, along with the code used to generate them, as a starting point for interested users. The data we provide represents an important real-world application of machine learning; to the best of our knowledge, no similar datasets are available.
LlCQWh8-pwK	A Procedural World Generation Framework for Systematic Evaluation of Continual Learning	https://openreview.net/forum?id=LlCQWh8-pwK	continual deep learning, synthetic data generation, continual learning benchmarks, computer vision, catastrophic interference	Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes.
-I6La-GJS5b	Deep Credal Neural Network: Characterization of Imprecision Between Categories	https://openreview.net/forum?id=-I6La-GJS5b	Deep credal neural network, Uncertainty, Imprecision, Belief functions, Meta-category, Classification	Quantification and reduction of uncertainty in deep learning techniques have received much attention but ignored how to characterize the imprecision caused by such uncertainty. In some tasks, we prefer to obtain an imprecise result rather than being willing or unable to bear the cost of an error. For this purpose, we present a deep credal neural network (DCNN) based on the theory of belief functions, aiming to assign samples that are indistinguishable for specific categories to the union of these, called meta-category. In DCNN, a designed mechanism assigns multiple labels to some training samples to constrain the known loss functions. Once assigned, it indicates that these samples may be in an overlapping region of different categories, or the original label is wrong. Afterward, the training labels are reconstructed and therefore classify the test samples. Once assigned to any meta-category, the prediction of this test sample is imprecise. Experiments based on some remarkable networks have shown that DCNN can not only improve accuracy but also reasonably characterize imprecision both in the training and test sets.
e9P6bypUFd	A Realistic Simulation Framework for Learning with Label Noise	https://openreview.net/forum?id=e9P6bypUFd	noisy labels, simulation, datasets, rater features	We propose a simulation framework for generating realistic instance-dependent noisy labels via a pseudo-labeling paradigm. We show that this framework generates synthetic noisy labels that exhibit important characteristics of the label noise in practical settings. Equipped with controllable label noise, we study the negative impact of noisy labels across a few realistic settings to understand when label noise is more problematic. Additionally, with the availability of annotator information from our simulation framework, we propose a new technique, Label Quality Model (LQM), that leverages annotator features to predict and correct against noisy labels. We show that by adding LQM as a label correction step before applying existing noisy label techniques, we can further improve the models' performance.
7l1Ygs3Bamw	CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review	https://openreview.net/forum?id=7l1Ygs3Bamw	law, legal nlp	Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.
1vC5GFOXuhM	CCNLab: A Benchmarking Framework for Computational Cognitive Neuroscience	https://openreview.net/forum?id=1vC5GFOXuhM	benchmark, computational neuroscience, cognitive neuroscience, classical conditioning	CCNLab is a benchmark for evaluating computational neuroscience models on empirical data. With classical conditioning as a case study, it includes a collection of seminal experiments in classical conditioning written in a domain-specific language, a common API for simulating different models on these experiments, and tools for visualizing and comparing the simulated data from the models with the empirical data. CCNLab is broad, covering many different phenomena; flexible, allowing the straightforward addition of new experiments; and easy to use, so researchers can focus on developing better models. We envision CCNLab as a testbed for unifying computational theories of learning in the brain. We also hope that it can broadly accelerate neuroscience research and facilitate interaction between the fields of neuroscience, psychology, and artificial intelligence.
7FHnnENUG0	Modeling Worlds in Text	https://openreview.net/forum?id=7FHnnENUG0	World Models, Text-based Games, Natural Language Processing, Knowledge Graphs	We provide a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives---or text-adventure games---are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects---each with their own unique descriptions---providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. Our dataset provides 24198 mappings between rich natural language observations and: (1) knowledge graphs that reflect the world state in the form of a map; (2) natural language actions that are guaranteed to cause a change in that particular world state. The training data is collected across 27 games in multiple genres and contains a further 7836 heldout instances over 9 additional games in the test set. We further provide baseline models using rules-based, question-answering, and sequence learning approaches in addition to an analysis of the data and corresponding learning tasks.
